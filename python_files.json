{
    "images_modified_raw.py": "import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import exposure\nimport random\nimport datetime\n\n# Constants\nRAW_AUDIO_PATH = \"./left.raw\"\nENCRYPTED_RAW_DIR = \"./modified_raw\"  # Now processing raw files\nOUTPUT_DIR = \"./adaptive_histogram_results\"\nBIT_FLIP_OUTPUT_DIR = \"./bit_flip_modified\"\nNUM_BIT_FLIP_FILES = 100\nMIN_BIT_FLIP_PERCENTAGE = 0.001\nMAX_BIT_FLIP_PERCENTAGE = 0.1\n\ndef read_raw_audio(file_path):\n    \"\"\"Read raw audio file as a NumPy array.\"\"\"\n    with open(file_path, \"rb\") as f:\n        return np.frombuffer(f.read(), dtype=np.uint8)\n\ndef compute_histogram(data):\n    \"\"\"Compute frequency histogram of byte data (0-255).\"\"\"\n    return np.bincount(data, minlength=256)\n\ndef apply_adaptive_histogram_equalization(hist, clip_limit=0.03):\n    \"\"\"Apply adaptive histogram equalization to the given histogram.\"\"\"\n    hist_normalized = hist.astype(np.float32) / hist.sum()\n    hist_equalized = exposure.equalize_adapthist(hist_normalized.reshape(16, 16), clip_limit=clip_limit).flatten()\n    hist_equalized = (hist_equalized * hist.sum()).astype(np.int32)\n    return hist_equalized\n\ndef plot_histogram(hist, title, output_path):\n    \"\"\"Plot the given histogram.\"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.bar(range(256), hist, color=\"blue\", alpha=0.7)\n    plt.title(title)\n    plt.xlabel(\"Byte Value\")\n    plt.ylabel(\"Frequency\")\n    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n    plt.savefig(output_path)\n    plt.close()\n\ndef flip_bits_in_file(input_file, output_file, flip_percentage=0.01):\n    \"\"\"Reads a raw audio file, flips bits randomly, and saves the modified data.\"\"\"\n    with open(input_file, 'rb') as f_in:\n        data = bytearray(f_in.read())\n\n    num_bytes_to_flip = int(len(data) * flip_percentage)\n    \n    for _ in range(num_bytes_to_flip):\n        byte_index = random.randint(0, len(data) - 1)\n        bit_index = random.randint(0, 7)\n        data[byte_index] ^= (1 << bit_index)\n\n    with open(output_file, 'wb') as f_out:\n        f_out.write(data)\n\ndef generate_random_flipped_files(input_file, output_dir, num_files, min_percentage, max_percentage):\n    \"\"\"Generates multiple versions of the input file with random bit flips.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    base_filename = os.path.splitext(os.path.basename(input_file))[0]\n\n    for i in range(num_files):\n        percentage = random.uniform(min_percentage, max_percentage)\n        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        output_filename = f\"{base_filename}_flipped_{percentage:.4f}_{i+1:03d}_{timestamp}.raw\"\n        output_file = os.path.join(output_dir, output_filename)\n        flip_bits_in_file(input_file, output_file, percentage)\n        print(f\"Generated: {output_file}\")\n\ndef analyze_bit_flipped_files(bit_flip_output_dir, output_dir):\n    \"\"\"Analyze all bit-flipped files in the given directory.\"\"\"\n    for filename in os.listdir(bit_flip_output_dir):\n        if filename.endswith(\".raw\"):\n            filepath = os.path.join(bit_flip_output_dir, filename)\n            flipped_data = read_raw_audio(filepath)\n            flipped_hist = compute_histogram(flipped_data)\n            flipped_hist_equalized = apply_adaptive_histogram_equalization(flipped_hist)\n\n            basename = os.path.splitext(filename)[0]\n            plot_path = os.path.join(output_dir, f\"{basename}_frequency_distribution.png\")\n            equalized_plot_path = os.path.join(output_dir, f\"equalized_{basename}_frequency_distribution.png\")\n\n            plot_histogram(flipped_hist, f\"Bit-Flipped ({basename}) Frequency Distribution\", plot_path)\n            plot_histogram(flipped_hist_equalized, f\"Equalized Bit-Flipped ({basename}) Frequency Distribution\", equalized_plot_path)\n\ndef main():\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    os.makedirs(BIT_FLIP_OUTPUT_DIR, exist_ok=True)\n\n    # Read raw audio data\n    raw_data = read_raw_audio(RAW_AUDIO_PATH)\n    raw_hist = compute_histogram(raw_data)\n    raw_hist_equalized = apply_adaptive_histogram_equalization(raw_hist)\n\n    # Plot raw audio histograms\n    plot_histogram(raw_hist, \"Raw Audio Frequency Distribution\", os.path.join(OUTPUT_DIR, \"raw_frequency_distribution.png\"))\n    plot_histogram(raw_hist_equalized, \"Equalized Raw Audio Frequency Distribution\", os.path.join(OUTPUT_DIR, \"equalized_raw_frequency_distribution.png\"))\n\n    # Generate and analyze bit-flipped files\n    generate_random_flipped_files(RAW_AUDIO_PATH, BIT_FLIP_OUTPUT_DIR, NUM_BIT_FLIP_FILES, MIN_BIT_FLIP_PERCENTAGE, MAX_BIT_FLIP_PERCENTAGE)\n    analyze_bit_flipped_files(BIT_FLIP_OUTPUT_DIR, OUTPUT_DIR)\n\n    # Process each encrypted RAW file\n    encrypted_raw_dir = ENCRYPTED_RAW_DIR\n    for filename in os.listdir(encrypted_raw_dir):\n        if filename.endswith(\".raw\"):\n            filepath = os.path.join(encrypted_raw_dir, filename)\n            encrypted_data = read_raw_audio(filepath)\n            encrypted_hist = compute_histogram(encrypted_data)\n            encrypted_hist_equalized = apply_adaptive_histogram_equalization(encrypted_hist)\n\n            basename = os.path.splitext(filename)[0]\n            plot_path = os.path.join(OUTPUT_DIR, f\"{basename}_frequency_distribution.png\")\n            equalized_plot_path = os.path.join(OUTPUT_DIR, f\"equalized_{basename}_frequency_distribution.png\")\n\n            plot_histogram(encrypted_hist, f\"Encrypted Audio ({basename}) Frequency Distribution\", plot_path)\n            plot_histogram(encrypted_hist_equalized, f\"Equalized Encrypted Audio ({basename}) Frequency Distribution\", equalized_plot_path)\n\n    print(f\"Processing results saved to {OUTPUT_DIR}\")\n\nif __name__ == \"__main__\":\n    main()",
    "analyzz.py": "# analyzz.py\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import entropy\nimport glob\nfrom handle_raw_audio import read_raw_audio\n\n# Constants\nRAW_AUDIO_PATH = \"./left.raw\"\nENCRYPTED_RAW_DIR = \"./modified_raw\"\nOUTPUT_DIR = \"./decryption_analysis_2\"\n\ndef compute_histogram(data):\n    \"\"\"Compute frequency histogram of byte data (0-255).\"\"\"\n    return np.bincount(data, minlength=256)\n\ndef plot_histogram(hist, title, output_path):\n    \"\"\"Plot the given histogram with hex values on x-axis.\"\"\"\n    plt.figure(figsize=(12, 6))\n    plt.bar(range(256), hist, color=\"blue\", alpha=0.7)\n    plt.title(title)\n    \n    # Add hex labels at key positions\n    hex_positions = list(range(0, 256, 16))\n    hex_labels = [f\"{i:02X}\" for i in hex_positions]\n    plt.xticks(hex_positions, hex_labels)\n    \n    plt.xlabel(\"Byte Value (Hex)\")\n    plt.ylabel(\"Frequency\")\n    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n    plt.savefig(output_path)\n    plt.close()\n\ndef analyze_transformation_patterns(raw_data, encrypted_data):\n    \"\"\"Analyze possible transformation patterns between raw and encrypted data.\"\"\"\n    # Create a transformation mapping\n    transform_mapping = {}\n    inverse_mapping = {}\n    \n    # Find byte pairs that occur most frequently \n    min_length = min(len(raw_data), len(encrypted_data))\n    for i in range(min_length):\n        raw_byte = raw_data[i]\n        enc_byte = encrypted_data[i]\n        \n        if raw_byte not in transform_mapping:\n            transform_mapping[raw_byte] = {}\n        \n        if enc_byte not in transform_mapping[raw_byte]:\n            transform_mapping[raw_byte][enc_byte] = 0\n        \n        transform_mapping[raw_byte][enc_byte] += 1\n        \n        # Also track inverse mapping\n        if enc_byte not in inverse_mapping:\n            inverse_mapping[enc_byte] = {}\n        \n        if raw_byte not in inverse_mapping[enc_byte]:\n            inverse_mapping[enc_byte][raw_byte] = 0\n        \n        inverse_mapping[enc_byte][raw_byte] += 1\n    \n    # Determine most likely transformation rule\n    transformation_rule = {}\n    inverse_rule = {}\n    \n    for raw_byte, mappings in transform_mapping.items():\n        most_common_enc = max(mappings.items(), key=lambda x: x[1])[0]\n        transformation_rule[raw_byte] = most_common_enc\n    \n    for enc_byte, mappings in inverse_mapping.items():\n        most_common_raw = max(mappings.items(), key=lambda x: x[1])[0]\n        inverse_rule[enc_byte] = most_common_raw\n    \n    return transformation_rule, inverse_rule\n\ndef analyze_xor_key(raw_data, encrypted_data):\n    \"\"\"Check if the transformation could be a simple XOR with a key.\"\"\"\n    min_length = min(len(raw_data), len(encrypted_data))\n    \n    possible_keys = {}\n    for i in range(min_length):\n        key = raw_data[i] ^ encrypted_data[i]\n        if key not in possible_keys:\n            possible_keys[key] = 0\n        possible_keys[key] += 1\n    \n    # Sort by frequency\n    sorted_keys = sorted(possible_keys.items(), key=lambda x: x[1], reverse=True)\n    \n    # Test the top 5 keys\n    top_keys = [k for k, _ in sorted_keys[:5]]\n    results = []\n    \n    for key in top_keys:\n        decrypted = np.bitwise_xor(encrypted_data[:min_length], key)\n        raw_hist = compute_histogram(raw_data[:min_length])\n        decrypted_hist = compute_histogram(decrypted)\n        \n        # Use Jensen-Shannon divergence to compare histograms\n        similarity = 1.0 - entropy(raw_hist + 1, decrypted_hist + 1) / np.log(2)\n        results.append((key, similarity))\n    \n    return sorted(results, key=lambda x: x[1], reverse=True)\n\ndef test_byte_mapping(raw_data, encrypted_data, output_dir):\n    \"\"\"Test if there's a consistent byte-to-byte mapping.\"\"\"\n    min_length = min(len(raw_data), len(encrypted_data))\n    mapping = np.zeros((256, 256), dtype=np.int32)\n    \n    for i in range(min_length):\n        mapping[raw_data[i], encrypted_data[i]] += 1\n    \n    # Plot the mapping as a heatmap\n    plt.figure(figsize=(10, 10))\n    plt.imshow(np.log1p(mapping), cmap='viridis')\n    plt.colorbar(label='log(count + 1)')\n    plt.title('Byte Mapping Heatmap (Raw \u2192 Encrypted)')\n    plt.xlabel('Encrypted Byte')\n    plt.ylabel('Raw Byte')\n    \n    # Add some hex labels\n    hex_positions = list(range(0, 256, 32))\n    hex_labels = [f\"{i:02X}\" for i in hex_positions]\n    plt.xticks(hex_positions, hex_labels)\n    plt.yticks(hex_positions, hex_labels)\n    \n    plt.savefig(os.path.join(output_dir, \"byte_mapping_heatmap.png\"))\n    plt.close()\n    \n    # Find the most common mapping for each raw byte\n    forward_mapping = {}\n    for i in range(256):\n        if np.sum(mapping[i, :]) > 0:\n            forward_mapping[i] = np.argmax(mapping[i, :])\n    \n    return forward_mapping\n\ndef decrypt_with_mapping(encrypted_data, mapping):\n    \"\"\"Decrypt data using a byte-to-byte mapping.\"\"\"\n    decrypted = np.zeros_like(encrypted_data)\n    for i in range(len(encrypted_data)):\n        if encrypted_data[i] in mapping:\n            decrypted[i] = mapping[encrypted_data[i]]\n        else:\n            decrypted[i] = encrypted_data[i]  # Keep unchanged if no mapping\n    return decrypted\n\ndef analyze_modular_arithmetic(raw_data, encrypted_data):\n    \"\"\"Test if the transformation follows the pattern: enc = (raw + key) % 256 or enc = (raw - key) % 256.\"\"\"\n    min_length = min(len(raw_data), len(encrypted_data))\n    \n    # Cast to int32 to avoid overflow during subtraction\n    raw_data_int32 = raw_data.astype(np.int32)\n    encrypted_data_int32 = encrypted_data.astype(np.int32)\n    \n    # Test addition\n    add_keys = {}\n    for i in range(min_length): \n        key = (encrypted_data_int32[i] - raw_data_int32[i]) % 256\n        if key not in add_keys:\n            add_keys[key] = 0\n        add_keys[key] += 1\n    \n    # Test subtraction\n    sub_keys = {}\n    for i in range(min_length):\n        key = (raw_data_int32[i] - encrypted_data_int32[i]) % 256\n        if key not in sub_keys:\n            sub_keys[key] = 0\n        sub_keys[key] += 1\n    \n    # Get top candidates\n    top_add_keys = sorted(add_keys.items(), key=lambda x: x[1], reverse=True)[:3]\n    top_sub_keys = sorted(sub_keys.items(), key=lambda x: x[1], reverse=True)[:3]\n    \n    return {\n        \"addition\": top_add_keys,\n        \"subtraction\": top_sub_keys\n    }\n\ndef test_decryption_methods(raw_data, encrypted_data, output_dir):\n    \"\"\"Test different decryption methods and evaluate results.\"\"\"\n    results = []\n    \n    # 1. Test XOR keys\n    xor_results = analyze_xor_key(raw_data, encrypted_data)\n    best_xor_key, best_xor_similarity = xor_results[0]\n    \n    decrypted_xor = np.bitwise_xor(encrypted_data, best_xor_key)\n    results.append((\"XOR\", best_xor_key, best_xor_similarity, decrypted_xor))\n    \n    # 2. Test modular arithmetic\n    mod_results = analyze_modular_arithmetic(raw_data, encrypted_data)\n    \n    # Addition (decryption is subtraction)\n    best_add_key, best_add_count = mod_results[\"addition\"][0]\n    decrypted_add = (encrypted_data - best_add_key) % 256\n    add_similarity = 1.0 - entropy(compute_histogram(raw_data), compute_histogram(decrypted_add) + 1) / np.log(2)\n    results.append((\"Addition\", best_add_key, add_similarity, decrypted_add))\n    \n    # Subtraction (decryption is addition)\n    best_sub_key, best_sub_count = mod_results[\"subtraction\"][0]\n    decrypted_sub = (encrypted_data + best_sub_key) % 256\n    sub_similarity = 1.0 - entropy(compute_histogram(raw_data), compute_histogram(decrypted_sub) + 1) / np.log(2)\n    results.append((\"Subtraction\", best_sub_key, sub_similarity, decrypted_sub))\n    \n    # 3. Test byte mapping\n    forward_mapping = test_byte_mapping(raw_data, encrypted_data, output_dir)\n    \n    # Invert the mapping\n    inverse_mapping = {}\n    for raw_byte, enc_byte in forward_mapping.items():\n        inverse_mapping[enc_byte] = raw_byte\n    \n    decrypted_mapping = decrypt_with_mapping(encrypted_data, inverse_mapping)\n    mapping_similarity = 1.0 - entropy(compute_histogram(raw_data), compute_histogram(decrypted_mapping) + 1) / np.log(2)\n    results.append((\"Mapping\", len(inverse_mapping), mapping_similarity, decrypted_mapping))\n    \n    # Sort by similarity\n    results.sort(key=lambda x: x[2], reverse=True)\n    \n    # Plot histograms for comparison\n    raw_hist = compute_histogram(raw_data)\n    \n    for method, key, similarity, decrypted in results:\n        decrypted_hist = compute_histogram(decrypted)\n        \n        plt.figure(figsize=(12, 6))\n        plt.subplot(1, 2, 1)\n        plt.bar(range(256), raw_hist, color=\"blue\", alpha=0.5, label=\"Raw\")\n        plt.bar(range(256), decrypted_hist, color=\"red\", alpha=0.5, label=\"Decrypted\")\n        plt.legend()\n        plt.title(f\"{method} Decryption (Key: {key}, Similarity: {similarity:.4f})\")\n        \n        plt.subplot(1, 2, 2)\n        plt.bar(range(256), np.abs(raw_hist - decrypted_hist), color=\"green\", alpha=0.7)\n        plt.title(\"Absolute Difference\")\n        \n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, f\"decryption_{method}_key_{key}.png\"))\n        plt.close()\n        \n        # Save the decrypted data\n        with open(os.path.join(output_dir, f\"decrypted_{method}_key_{key}.raw\"), \"wb\") as f:\n            f.write(decrypted.tobytes())\n    \n    return results\n\ndef main():\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    \n    # Read raw audio data\n    raw_data = read_raw_audio(RAW_AUDIO_PATH)\n    raw_hist = compute_histogram(raw_data)\n    plot_histogram(raw_hist, \"Raw Audio Frequency Distribution\", \n                 os.path.join(OUTPUT_DIR, \"raw_frequency_distribution.png\"))\n    \n    # Process the first encrypted file for initial analysis\n    encrypted_files = glob.glob(os.path.join(ENCRYPTED_RAW_DIR, \"*.raw\"))\n    if not encrypted_files:\n        print(\"No encrypted files found!\")\n        return\n    \n    first_file = encrypted_files[0]\n    print(f\"Analyzing primary file: {first_file}\")\n    encrypted_data = read_raw_audio(first_file)\n    encrypted_hist = compute_histogram(encrypted_data)\n    plot_histogram(encrypted_hist, f\"Encrypted Audio Frequency Distribution\", \n                 os.path.join(OUTPUT_DIR, \"encrypted_frequency_distribution.png\"))\n    \n    # Analyze transformation patterns\n    print(\"Testing possible decryption methods...\")\n    decryption_results = test_decryption_methods(raw_data, encrypted_data, OUTPUT_DIR)\n    \n    # Report results\n    print(\"\\nDecryption Method Results (Sorted by Similarity):\")\n    for method, key, similarity, _ in decryption_results:\n        print(f\"Method: {method}, Key: {key}, Similarity: {similarity:.4f}\")\n    \n    best_method, best_key, best_similarity, best_decrypted = decryption_results[0]\n    print(f\"\\nBest method: {best_method} with key {best_key} (Similarity: {best_similarity:.4f})\")\n    \n    # Apply the best method to all encrypted files\n    print(\"\\nApplying best decryption method to all encrypted files...\")\n    for encrypted_file in encrypted_files:\n        basename = os.path.splitext(os.path.basename(encrypted_file))[0]\n        encrypted_data = read_raw_audio(encrypted_file)\n        \n        # Apply the best decryption method\n        if best_method == \"XOR\":\n            decrypted = np.bitwise_xor(encrypted_data, best_key)\n        elif best_method == \"Addition\":\n            decrypted = (encrypted_data - best_key) % 256\n        elif best_method == \"Subtraction\":\n            decrypted = (encrypted_data + best_key) % 256\n        elif best_method == \"Mapping\":\n            # Recreate the inverse mapping for the best result\n            forward_mapping = test_byte_mapping(raw_data, read_raw_audio(first_file), OUTPUT_DIR)\n            inverse_mapping = {v: k for k, v in forward_mapping.items()}\n            decrypted = decrypt_with_mapping(encrypted_data, inverse_mapping)\n        \n        # Save the decrypted file\n        output_file = os.path.join(OUTPUT_DIR, f\"decrypted_{basename}.raw\")\n        with open(output_file, \"wb\") as f:\n            f.write(decrypted.tobytes())\n        \n        print(f\"Decrypted {basename} saved to {output_file}\")\n    \n    print(f\"\\nAll results saved to {OUTPUT_DIR}\")\n\nif __name__ == \"__main__\":\n    main()",
    "transition_matrix.py": "import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import cosine\nfrom scipy.stats import pearsonr\nimport glob\n\n# Constants\nRAW_AUDIO_PATH = \"./left.raw\"\nENCRYPTED_RAW_DIR = \"./modified_raw\"\nDECRYPTED_DIR = \"./decryption_analysis\"  # Directory where decrypted files are saved\nOUTPUT_DIR = \"./pattern_validation_results\"\n\ndef read_raw_audio(file_path):\n    \"\"\"Read raw audio file as a NumPy array.\"\"\"\n    with open(file_path, \"rb\") as f:\n        return np.frombuffer(f.read(), dtype=np.uint8)\n\ndef compute_histogram(data):\n    \"\"\"Compute frequency histogram of byte data (0-255).\"\"\"\n    return np.bincount(data, minlength=256)\n\ndef cosine_similarity(hist1, hist2):\n    \"\"\"Compute cosine similarity between two histograms.\"\"\"\n    return 1 - cosine(hist1, hist2)\n\ndef pearson_correlation(hist1, hist2):\n    \"\"\"Compute Pearson correlation coefficient between two histograms.\"\"\"\n    corr, _ = pearsonr(hist1, hist2)\n    return corr\n\ndef validate_decryption(raw_hist, decrypted_hist):\n    \"\"\"\n    Validate decryption by comparing histograms using cosine similarity and Pearson correlation.\n    Returns a dictionary with similarity metrics.\n    \"\"\"\n    metrics = {\n        \"cosine_similarity\": cosine_similarity(raw_hist, decrypted_hist),\n        \"pearson_correlation\": pearson_correlation(raw_hist, decrypted_hist)\n    }\n    return metrics\n\ndef analyze_byte_transformation_patterns(raw_data, encrypted_data):\n    \"\"\"\n    Analyze byte transformation patterns by computing transition probabilities.\n    Returns a 256x256 matrix where M[i][j] represents the probability of byte i transforming to byte j.\n    \"\"\"\n    min_length = min(len(raw_data), len(encrypted_data))\n    transition_matrix = np.zeros((256, 256), dtype=np.float32)\n\n    for i in range(min_length):\n        raw_byte = raw_data[i]\n        enc_byte = encrypted_data[i]\n        transition_matrix[raw_byte][enc_byte] += 1\n\n    # Normalize rows to get probabilities\n    row_sums = transition_matrix.sum(axis=1, keepdims=True)\n    row_sums[row_sums == 0] = 1  # Avoid division by zero\n    transition_matrix /= row_sums\n\n    return transition_matrix\n\ndef plot_transition_heatmap(matrix, title, output_path):\n    \"\"\"Plot a heatmap of the byte transition matrix.\"\"\"\n    plt.figure(figsize=(10, 10))\n    plt.imshow(matrix, cmap='viridis', interpolation='nearest')\n    plt.colorbar(label=\"Probability\")\n    plt.title(title)\n    plt.xlabel(\"Encrypted Byte\")\n    plt.ylabel(\"Raw Byte\")\n    plt.xticks(np.arange(0, 256, 32))\n    plt.yticks(np.arange(0, 256, 32))\n    plt.savefig(output_path)\n    plt.close()\n\ndef main():\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    # Read raw audio data\n    raw_data = read_raw_audio(RAW_AUDIO_PATH)\n    raw_hist = compute_histogram(raw_data)\n\n    # Process each encrypted RAW file\n    encrypted_files = glob.glob(os.path.join(ENCRYPTED_RAW_DIR, \"*.raw\"))\n    if not encrypted_files:\n        print(\"No encrypted files found!\")\n        return\n\n    results = []\n\n    for encrypted_file in encrypted_files:\n        basename = os.path.splitext(os.path.basename(encrypted_file))[0]\n        encrypted_data = read_raw_audio(encrypted_file)\n        encrypted_hist = compute_histogram(encrypted_data)\n\n        # Analyze byte transformation patterns\n        transition_matrix = analyze_byte_transformation_patterns(raw_data, encrypted_data)\n        heatmap_path = os.path.join(OUTPUT_DIR, f\"{basename}_transition_heatmap.png\")\n        plot_transition_heatmap(transition_matrix, f\"Byte Transition Heatmap ({basename})\", heatmap_path)\n\n        # Validate decrypted files\n        decrypted_files = glob.glob(os.path.join(DECRYPTED_DIR, f\"decrypted_{basename}*.raw\"))\n        for decrypted_file in decrypted_files:\n            decrypted_data = read_raw_audio(decrypted_file)\n            decrypted_hist = compute_histogram(decrypted_data)\n\n            # Compute validation metrics\n            metrics = validate_decryption(raw_hist, decrypted_hist)\n            results.append({\n                \"encrypted_file\": encrypted_file,\n                \"decrypted_file\": decrypted_file,\n                \"cosine_similarity\": metrics[\"cosine_similarity\"],\n                \"pearson_correlation\": metrics[\"pearson_correlation\"]\n            })\n\n    # Save results to a report\n    report_path = os.path.join(OUTPUT_DIR, \"validation_report.txt\")\n    with open(report_path, \"w\") as report:\n        report.write(\"Decryption Validation Report\\n\")\n        report.write(\"=\" * 50 + \"\\n\\n\")\n\n        for result in results:\n            report.write(f\"Encrypted File: {result['encrypted_file']}\\n\")\n            report.write(f\"Decrypted File: {result['decrypted_file']}\\n\")\n            report.write(f\"Cosine Similarity: {result['cosine_similarity']:.4f}\\n\")\n            report.write(f\"Pearson Correlation: {result['pearson_correlation']:.4f}\\n\")\n            report.write(\"-\" * 50 + \"\\n\")\n\n    print(f\"Validation results saved to {report_path}\")\n\nif __name__ == \"__main__\":\n    main()",
    "sliding_window.py": "import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats\nimport wave\n\n# Constants\nRAW_AUDIO_PATH = \"./left.raw\"\nENCRYPTED_WAVS_DIR = \"./modified_raw\"\nENTROPY_ANALYSIS_RESULTS_DIR = \"./entropy_analysis_results\"\n\ndef read_raw_audio(file_path):\n    \"\"\"Read raw audio file as a NumPy array.\"\"\"\n    with open(file_path, \"rb\") as f:\n        return np.frombuffer(f.read(), dtype=np.uint8)\n\ndef read_wav_audio(file_path):\n    \"\"\"Read WAV file's audio data as a NumPy array of uint8.\"\"\"\n    with wave.open(file_path, 'rb') as wav_file:\n        num_frames = wav_file.getnframes()\n        audio_bytes = wav_file.readframes(num_frames)\n        return np.frombuffer(audio_bytes, dtype=np.uint8)\n\ndef compute_histogram(data):\n    \"\"\"Compute frequency histogram of byte data (0-255).\"\"\"\n    return np.bincount(data, minlength=256)\n\ndef calculate_entropy(data):\n    \"\"\"Calculate entropy of the given byte data.\"\"\"\n    hist = np.bincount(data, minlength=256)\n    probs = hist / len(data)\n    probs = probs[probs > 0]  # Remove zero probabilities\n    return scipy.stats.entropy(probs)\n\ndef calculate_sliding_window_entropy(data, window_size, step_size):\n    \"\"\"Calculate entropy with a sliding window.\"\"\"\n    entropies = []\n    for i in range(0, len(data) - window_size + 1, step_size):\n        window = data[i:i + window_size]\n        entropy = calculate_entropy(window)\n        entropies.append(entropy)\n    return entropies\n\ndef plot_sliding_window_entropy(entropies, window_size, step_size, filename, output_dir):\n    \"\"\"Plot the sliding window entropy.\"\"\"\n    plt.figure(figsize=(12, 6))\n    plt.plot(entropies)\n    plt.title(f\"Sliding Window Entropy (Window: {window_size}, Step: {step_size}) - {filename}\")\n    plt.xlabel(\"Window Position\")\n    plt.ylabel(\"Entropy\")\n    plt.grid(True)\n    os.makedirs(output_dir, exist_ok=True) #added this line to make sure the directories are created.\n    plt.savefig(os.path.join(output_dir, f\"entropy_{filename}_window_{window_size}_step_{step_size}.png\"))\n    plt.close()\n\ndef main():\n    os.makedirs(ENTROPY_ANALYSIS_RESULTS_DIR, exist_ok=True)\n\n    # Calculate entropy for the raw audio\n    raw_data = read_raw_audio(RAW_AUDIO_PATH)\n    raw_entropy = calculate_entropy(raw_data)\n    print(f\"Entropy of raw audio: {raw_entropy:.4f}\")\n\n    # Calculate entropy for each encrypted WAV file\n    for filename in os.listdir(ENCRYPTED_WAVS_DIR):\n        if filename.endswith(\".wav\"):\n            filepath = os.path.join(ENCRYPTED_WAVS_DIR, filename)\n            encrypted_data = read_wav_audio(filepath)\n            encrypted_entropy = calculate_entropy(encrypted_data)\n            print(f\"Entropy of {filename}: {encrypted_entropy:.4f}\")\n\n            # Sliding window entropy analysis\n            window_sizes = [100, 500, 1000]\n            step_sizes = [50, 250, 500]\n\n            for window_size in window_sizes:\n                for step_size in step_sizes:\n                    entropies = calculate_sliding_window_entropy(encrypted_data, window_size, step_size)\n                    plot_sliding_window_entropy(entropies, window_size, step_size, os.path.splitext(filename)[0], ENTROPY_ANALYSIS_RESULTS_DIR)\n\n    print(f\"Entropy analysis results saved to {ENTROPY_ANALYSIS_RESULTS_DIR}\")\n\nif __name__ == \"__main__\":\n    main()",
    "entropy_analysis.py": "# entropy_analysis.py\nimport os\nimport numpy as np\nimport scipy.stats\nimport matplotlib.pyplot as plt\nfrom handle_raw_audio import read_raw_audio  # Import from handle_raw_audio\n\n# Constants\nENCRYPTED_RAW_DIR = \"./modified_raw\"  # Directory with encrypted raw files\nOUTPUT_DIR = \"./entropy_analysis_results\"\nWINDOW_SIZES = [100, 500, 1000]  # Different window sizes for entropy calculation\nSTEP_SIZES = [50, 250, 500]  # Different step sizes for sliding window\n\ndef compute_histogram(data):\n    \"\"\"Compute frequency histogram of byte data (0-255).\"\"\"\n    return np.bincount(data, minlength=256)\n\ndef calculate_entropy(data):\n    \"\"\"Calculate entropy of the given byte data.\"\"\"\n    hist = np.bincount(data, minlength=256)\n    probs = hist / len(data)\n    probs = probs[probs > 0]  # Remove zero probabilities\n    return scipy.stats.entropy(probs)\n\ndef calculate_sliding_window_entropy(data, window_size, step_size):\n    \"\"\"Calculate entropy with a sliding window.\"\"\"\n    entropies = []\n    for i in range(0, len(data) - window_size + 1, step_size):\n        window = data[i:i + window_size]\n        entropy = calculate_entropy(window)\n        entropies.append(entropy)\n    return entropies\n\ndef plot_sliding_window_entropy(entropies, window_size, step_size, filename, output_dir):\n    \"\"\"Plot the sliding window entropy.\"\"\"\n    plt.figure(figsize=(12, 6))\n    plt.plot(entropies)\n    plt.title(f\"Sliding Window Entropy (Window: {window_size}, Step: {step_size}) - {filename}\")\n    plt.xlabel(\"Window Position\")\n    plt.ylabel(\"Entropy\")\n    plt.grid(True)\n    os.makedirs(output_dir, exist_ok=True)\n    plt.savefig(os.path.join(output_dir, f\"entropy_{filename}_window_{window_size}_step_{step_size}.png\"))\n    plt.close()\n\ndef main():\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    for filename in os.listdir(ENCRYPTED_RAW_DIR):\n        if filename.endswith(\".raw\"):\n            filepath = os.path.join(ENCRYPTED_RAW_DIR, filename)\n            data = read_raw_audio(filepath)\n\n            for window_size in WINDOW_SIZES:\n                for step_size in STEP_SIZES:\n                    entropies = calculate_sliding_window_entropy(data, window_size, step_size)\n                    plot_sliding_window_entropy(entropies, window_size, step_size, os.path.splitext(filename)[0], OUTPUT_DIR)\n\n    print(f\"Entropy analysis results saved to {OUTPUT_DIR}\")\n\nif __name__ == \"__main__\":\n    main()",
    "analyze2.py": "import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import entropy\nimport glob\nfrom extract_audio_features import extract_audio_features\n\n# Constants\nRAW_AUDIO_PATH = \"./left.raw\"\nENCRYPTED_RAW_DIR = \"./modified_raw\"\nOUTPUT_DIR = \"./decryption_analysis\"\n\ndef read_raw_audio(file_path):\n    \"\"\"Read raw audio file as a NumPy array.\"\"\"\n    with open(file_path, \"rb\") as f:\n        return np.frombuffer(f.read(), dtype=np.uint8)\n\ndef compute_histogram(data):\n    \"\"\"Compute frequency histogram of byte data (0-255).\"\"\"\n    return np.bincount(data, minlength=256)\n\ndef plot_histogram(hist, title, output_path):\n    \"\"\"Plot the given histogram with hex values on x-axis.\"\"\"\n    plt.figure(figsize=(12, 6))\n    plt.bar(range(256), hist, color=\"blue\", alpha=0.7)\n    plt.title(title)\n    \n    # Add hex labels at key positions\n    hex_positions = list(range(0, 256, 16))\n    hex_labels = [f\"{i:02X}\" for i in hex_positions]\n    plt.xticks(hex_positions, hex_labels)\n    \n    plt.xlabel(\"Byte Value (Hex)\")\n    plt.ylabel(\"Frequency\")\n    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n    plt.savefig(output_path)\n    plt.close()\n\ndef analyze_transformation_patterns(raw_data, encrypted_data):\n    \"\"\"Analyze possible transformation patterns between raw and encrypted data.\"\"\"\n    # Create a transformation mapping\n    transform_mapping = {}\n    inverse_mapping = {}\n    \n    # Find byte pairs that occur most frequently\n    min_length = min(len(raw_data), len(encrypted_data))\n    for i in range(min_length):\n        raw_byte = raw_data[i]\n        enc_byte = encrypted_data[i]\n        \n        if raw_byte not in transform_mapping:\n            transform_mapping[raw_byte] = {}\n        \n        if enc_byte not in transform_mapping[raw_byte]:\n            transform_mapping[raw_byte][enc_byte] = 0\n        \n        transform_mapping[raw_byte][enc_byte] += 1\n        \n        # Also track inverse mapping\n        if enc_byte not in inverse_mapping:\n            inverse_mapping[enc_byte] = {}\n        \n        if raw_byte not in inverse_mapping[enc_byte]:\n            inverse_mapping[enc_byte][raw_byte] = 0\n        \n        inverse_mapping[enc_byte][raw_byte] += 1\n    \n    # Determine most likely transformation rule\n    transformation_rule = {}\n    inverse_rule = {}\n    \n    for raw_byte, mappings in transform_mapping.items():\n        most_common_enc = max(mappings.items(), key=lambda x: x[1])[0]\n        transformation_rule[raw_byte] = most_common_enc\n    \n    for enc_byte, mappings in inverse_mapping.items():\n        most_common_raw = max(mappings.items(), key=lambda x: x[1])[0]\n        inverse_rule[enc_byte] = most_common_raw\n    \n    return transformation_rule, inverse_rule\n\ndef analyze_xor_key(raw_data, encrypted_data):\n    \"\"\"Check if the transformation could be a simple XOR with a key.\"\"\"\n    min_length = min(len(raw_data), len(encrypted_data))\n    \n    possible_keys = {}\n    for i in range(min_length):\n        key = raw_data[i] ^ encrypted_data[i]\n        if key not in possible_keys:\n            possible_keys[key] = 0\n        possible_keys[key] += 1\n    \n    # Sort by frequency\n    sorted_keys = sorted(possible_keys.items(), key=lambda x: x[1], reverse=True)\n    \n    # Test the top 5 keys\n    top_keys = [k for k, _ in sorted_keys[:5]]\n    results = []\n    \n    for key in top_keys:\n        decrypted = np.bitwise_xor(encrypted_data[:min_length], key)\n        raw_hist = compute_histogram(raw_data[:min_length])\n        decrypted_hist = compute_histogram(decrypted)\n        \n        # Use Jensen-Shannon divergence to compare histograms\n        similarity = 1.0 - entropy(raw_hist + 1, decrypted_hist + 1) / np.log(2)\n        results.append((key, similarity))\n    \n    return sorted(results, key=lambda x: x[1], reverse=True)\n\ndef test_byte_mapping(raw_data, encrypted_data, output_dir):\n    \"\"\"Test if there's a consistent byte-to-byte mapping.\"\"\"\n    min_length = min(len(raw_data), len(encrypted_data))\n    mapping = np.zeros((256, 256), dtype=np.int32)\n    \n    for i in range(min_length):\n        mapping[raw_data[i], encrypted_data[i]] += 1\n    \n    # Plot the mapping as a heatmap\n    plt.figure(figsize=(10, 10))\n    plt.imshow(np.log1p(mapping), cmap='viridis')\n    plt.colorbar(label='log(count + 1)')\n    plt.title('Byte Mapping Heatmap (Raw \u2192 Encrypted)')\n    plt.xlabel('Encrypted Byte')\n    plt.ylabel('Raw Byte')\n    \n    # Add some hex labels\n    hex_positions = list(range(0, 256, 32))\n    hex_labels = [f\"{i:02X}\" for i in hex_positions]\n    plt.xticks(hex_positions, hex_labels)\n    plt.yticks(hex_positions, hex_labels)\n    \n    plt.savefig(os.path.join(output_dir, \"byte_mapping_heatmap.png\"))\n    plt.close()\n    \n    # Find the most common mapping for each raw byte\n    forward_mapping = {}\n    for i in range(256):\n        if np.sum(mapping[i, :]) > 0:\n            forward_mapping[i] = np.argmax(mapping[i, :])\n    \n    return forward_mapping\n\ndef decrypt_with_mapping(encrypted_data, mapping):\n    \"\"\"Decrypt data using a byte-to-byte mapping.\"\"\"\n    decrypted = np.zeros_like(encrypted_data)\n    for i in range(len(encrypted_data)):\n        if encrypted_data[i] in mapping:\n            decrypted[i] = mapping[encrypted_data[i]]\n        else:\n            decrypted[i] = encrypted_data[i]  # Keep unchanged if no mapping\n    return decrypted\n\ndef analyze_modular_arithmetic(raw_data, encrypted_data):\n    \"\"\"Test if the transformation follows the pattern: enc = (raw + key) % 256 or enc = (raw - key) % 256.\"\"\"\n    min_length = min(len(raw_data), len(encrypted_data))\n    \n    # Cast to int32 to avoid overflow during subtraction\n    raw_data_int32 = raw_data.astype(np.int32)\n    encrypted_data_int32 = encrypted_data.astype(np.int32)\n    \n    # Test addition\n    add_keys = {}\n    for i in range(min_length): \n        key = (encrypted_data_int32[i] - raw_data_int32[i]) % 256\n        if key not in add_keys:\n            add_keys[key] = 0\n        add_keys[key] += 1\n    \n    # Test subtraction\n    sub_keys = {}\n    for i in range(min_length):\n        key = (raw_data_int32[i] - encrypted_data_int32[i]) % 256\n        if key not in sub_keys:\n            sub_keys[key] = 0\n        sub_keys[key] += 1\n    \n    # Get top candidates\n    top_add_keys = sorted(add_keys.items(), key=lambda x: x[1], reverse=True)[:3]\n    top_sub_keys = sorted(sub_keys.items(), key=lambda x: x[1], reverse=True)[:3]\n    \n    return {\n        \"addition\": top_add_keys,\n        \"subtraction\": top_sub_keys\n    }\n\ndef test_decryption_methods(raw_data, encrypted_data, output_dir):\n    \"\"\"Test different decryption methods and evaluate results.\"\"\"\n    results = []\n    \n    # 1. Test XOR keys\n    xor_results = analyze_xor_key(raw_data, encrypted_data)\n    best_xor_key, best_xor_similarity = xor_results[0]\n    \n    decrypted_xor = np.bitwise_xor(encrypted_data, best_xor_key)\n    results.append((\"XOR\", best_xor_key, best_xor_similarity, decrypted_xor))\n    \n    # 2. Test modular arithmetic\n    mod_results = analyze_modular_arithmetic(raw_data, encrypted_data)\n    \n    # Addition (decryption is subtraction)\n    best_add_key, best_add_count = mod_results[\"addition\"][0]\n    decrypted_add = (encrypted_data - best_add_key) % 256\n    add_similarity = 1.0 - entropy(compute_histogram(raw_data), compute_histogram(decrypted_add) + 1) / np.log(2)\n    results.append((\"Addition\", best_add_key, add_similarity, decrypted_add))\n    \n    # Subtraction (decryption is addition)\n    best_sub_key, best_sub_count = mod_results[\"subtraction\"][0]\n    decrypted_sub = (encrypted_data + best_sub_key) % 256\n    sub_similarity = 1.0 - entropy(compute_histogram(raw_data), compute_histogram(decrypted_sub) + 1) / np.log(2)\n    results.append((\"Subtraction\", best_sub_key, sub_similarity, decrypted_sub))\n    \n    # 3. Test byte mapping\n    forward_mapping = test_byte_mapping(raw_data, encrypted_data, output_dir)\n    \n    # Invert the mapping\n    inverse_mapping = {}\n    for raw_byte, enc_byte in forward_mapping.items():\n        inverse_mapping[enc_byte] = raw_byte\n    \n    decrypted_mapping = decrypt_with_mapping(encrypted_data, inverse_mapping)\n    mapping_similarity = 1.0 - entropy(compute_histogram(raw_data), compute_histogram(decrypted_mapping) + 1) / np.log(2)\n    results.append((\"Mapping\", len(inverse_mapping), mapping_similarity, decrypted_mapping))\n    \n    # Sort by similarity\n    results.sort(key=lambda x: x[2], reverse=True)\n    \n    # Plot histograms for comparison\n    raw_hist = compute_histogram(raw_data)\n    \n    for method, key, similarity, decrypted in results:\n        decrypted_hist = compute_histogram(decrypted)\n        \n        plt.figure(figsize=(12, 6))\n        plt.subplot(1, 2, 1)\n        plt.bar(range(256), raw_hist, color=\"blue\", alpha=0.5, label=\"Raw\")\n        plt.bar(range(256), decrypted_hist, color=\"red\", alpha=0.5, label=\"Decrypted\")\n        plt.legend()\n        plt.title(f\"{method} Decryption (Key: {key}, Similarity: {similarity:.4f})\")\n        \n        plt.subplot(1, 2, 2)\n        plt.bar(range(256), np.abs(raw_hist - decrypted_hist), color=\"green\", alpha=0.7)\n        plt.title(\"Absolute Difference\")\n        \n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, f\"decryption_{method}_key_{key}.png\"))\n        plt.close()\n        \n        # Save the decrypted data\n        with open(os.path.join(output_dir, f\"decrypted_{method}_key_{key}.raw\"), \"wb\") as f:\n            f.write(decrypted.tobytes())\n    \n    return results\n\ndef main():\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    \n    # Read raw audio data\n    raw_data = read_raw_audio(RAW_AUDIO_PATH)\n    raw_hist = compute_histogram(raw_data)\n    plot_histogram(raw_hist, \"Raw Audio Frequency Distribution\", \n                 os.path.join(OUTPUT_DIR, \"raw_frequency_distribution.png\"))\n    \n    # Process the first encrypted file for initial analysis\n    encrypted_files = glob.glob(os.path.join(ENCRYPTED_RAW_DIR, \"*.raw\"))\n    if not encrypted_files:\n        print(\"No encrypted files found!\")\n        return\n    \n    first_file = encrypted_files[0]\n    print(f\"Analyzing primary file: {first_file}\")\n    encrypted_data = read_raw_audio(first_file)\n    encrypted_hist = compute_histogram(encrypted_data)\n    plot_histogram(encrypted_hist, f\"Encrypted Audio Frequency Distribution\", \n                 os.path.join(OUTPUT_DIR, \"encrypted_frequency_distribution.png\"))\n    \n    # Analyze transformation patterns\n    print(\"Testing possible decryption methods...\")\n    decryption_results = test_decryption_methods(raw_data, encrypted_data, OUTPUT_DIR)\n    \n    # Report results\n    print(\"\\nDecryption Method Results (Sorted by Similarity):\")\n    for method, key, similarity, _ in decryption_results:\n        print(f\"Method: {method}, Key: {key}, Similarity: {similarity:.4f}\")\n    \n    best_method, best_key, best_similarity, best_decrypted = decryption_results[0]\n    print(f\"\\nBest method: {best_method} with key {best_key} (Similarity: {best_similarity:.4f})\")\n    \n    # Apply the best method to all encrypted files\n    print(\"\\nApplying best decryption method to all encrypted files...\")\n    for encrypted_file in encrypted_files:\n        basename = os.path.splitext(os.path.basename(encrypted_file))[0]\n        encrypted_data = read_raw_audio(encrypted_file)\n        \n        # Apply the best decryption method\n        if best_method == \"XOR\":\n            decrypted = np.bitwise_xor(encrypted_data, best_key)\n        elif best_method == \"Addition\":\n            decrypted = (encrypted_data - best_key) % 256\n        elif best_method == \"Subtraction\":\n            decrypted = (encrypted_data + best_key) % 256\n        elif best_method == \"Mapping\":\n            # Recreate the inverse mapping for the best result\n            forward_mapping = test_byte_mapping(raw_data, read_raw_audio(first_file), OUTPUT_DIR)\n            inverse_mapping = {v: k for k, v in forward_mapping.items()}\n            decrypted = decrypt_with_mapping(encrypted_data, inverse_mapping)\n        \n        # Save the decrypted file\n        output_file = os.path.join(OUTPUT_DIR, f\"decrypted_{basename}.raw\")\n        with open(output_file, \"wb\") as f:\n            f.write(decrypted.tobytes())\n        \n        print(f\"Decrypted {basename} saved to {output_file}\")\n    \n    print(f\"\\nAll results saved to {OUTPUT_DIR}\")\n\nif __name__ == \"__main__\":\n    main()",
    "create_variable_raw.py": "import random\nimport os\nimport datetime\n\ndef flip_bits_in_file(input_file, output_file, flip_percentage=0.01):\n    \"\"\"\n    Reads a raw audio file, flips bits randomly, and saves the modified data.\n\n    Args:\n        input_file (str): Path to the input raw audio file.\n        output_file (str): Path to save the modified raw audio file.\n        flip_percentage (float): Approximate percentage of bits to flip (0.0 to 1.0).\n    \"\"\"\n\n    with open(input_file, 'rb') as f_in:\n        data = bytearray(f_in.read())\n\n    num_bytes_to_flip = int(len(data) * flip_percentage)\n    \n    for _ in range(num_bytes_to_flip):\n        byte_index = random.randint(0, len(data) - 1)\n        bit_index = random.randint(0, 7)  # 8 bits in a byte\n\n        # Flip the bit\n        data[byte_index] ^= (1 << bit_index)\n\n    with open(output_file, 'wb') as f_out:\n        f_out.write(data)\n\ndef generate_random_flipped_files(input_file, output_dir, num_files, min_percentage=0.001, max_percentage=0.1):\n    \"\"\"\n    Generates multiple versions of the input file with random bit flips,\n    using a random percentage within a given range.\n\n    Args:\n        input_file (str): Path to the input raw audio file.\n        output_dir (str): Directory to save the generated files.\n        num_files (int): Number of files to generate.\n        min_percentage (float): Minimum percentage of bits to flip (0.0 to 1.0).\n        max_percentage (float): Maximum percentage of bits to flip (0.0 to 1.0).\n    \"\"\"\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    base_filename = os.path.splitext(os.path.basename(input_file))[0]\n\n    for i in range(num_files):\n        percentage = random.uniform(min_percentage, max_percentage)  # Generate random percentage\n        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        output_filename = f\"{base_filename}_flipped_{percentage:.4f}_{i+1:03d}_{timestamp}.raw\"\n        output_file = os.path.join(output_dir, output_filename)\n        flip_bits_in_file(input_file, output_file, percentage)\n        print(f\"Generated: {output_file}\")\n\n# --- Example Usage ---\ninput_file = \"right.raw\"\noutput_dir = \"modified_raw\"\n\nnum_files = 5000  # Generate 50 files with random percentages\nmin_percentage = 0.001  # Minimum percentage to flip\nmax_percentage = 0.1    # Maximum percentage to flip\n\ngenerate_random_flipped_files(input_file, output_dir, num_files, min_percentage, max_percentage)",
    "adaptive_histogram_equalization.py": "# adaptive_histogram_equalization.py\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import exposure\nimport glob\nfrom handle_raw_audio import read_raw_audio, extract_audio_features  # Import the new function\n\n# Constants\nRAW_AUDIO_PATH = \"./left.raw\"\nENCRYPTED_RAW_DIR = \"./modified_raw\"\nOUTPUT_DIR = \"./adaptive_histogram_results\"\n\ndef compute_histogram(data):\n    \"\"\"Compute frequency histogram of byte data (0-255).\"\"\"\n    return np.bincount(data, minlength=256)\n\ndef apply_adaptive_histogram_equalization(hist, clip_limit=0.03):\n    \"\"\"Apply adaptive histogram equalization to the given histogram.\"\"\"\n    hist_normalized = hist.astype(np.float32) / hist.sum()\n    hist_equalized = exposure.equalize_adapthist(hist_normalized.reshape(16, 16), clip_limit=clip_limit).flatten()\n    hist_equalized = (hist_equalized * hist.sum()).astype(np.int32)\n    return hist_equalized\n\ndef plot_histogram(hist, title, output_path):\n    \"\"\"Plot the given histogram.\"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.bar(range(256), hist, color=\"blue\", alpha=0.7)\n    plt.title(title)\n    plt.xlabel(\"Byte Value\")\n    plt.ylabel(\"Frequency\")\n    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n    plt.savefig(output_path)\n    plt.close()\n\ndef main():\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    # Read and process raw audio data\n    raw_data = read_raw_audio(RAW_AUDIO_PATH)\n    raw_hist = compute_histogram(raw_data)\n    raw_hist_equalized = apply_adaptive_histogram_equalization(raw_hist)\n\n    # Extract features from raw audio\n    raw_features = extract_audio_features(RAW_AUDIO_PATH)\n    if raw_features:\n        print(f\"Raw Audio Features: {raw_features}\")\n\n    # Process each encrypted RAW file\n    encrypted_files = glob.glob(os.path.join(ENCRYPTED_RAW_DIR, \"*.raw\"))\n    for encrypted_file in encrypted_files:\n        encrypted_data = read_raw_audio(encrypted_file)\n        encrypted_hist = compute_histogram(encrypted_data)\n        encrypted_hist_equalized = apply_adaptive_histogram_equalization(encrypted_hist)\n\n        # Extract features from encrypted audio\n        encrypted_features = extract_audio_features(encrypted_file)\n        if encrypted_features:\n            print(f\"Encrypted Audio Features ({os.path.basename(encrypted_file)}): {encrypted_features}\")\n\n        # Generate plot filenames with base name\n        basename = os.path.splitext(os.path.basename(encrypted_file))[0]\n        plot_path = os.path.join(OUTPUT_DIR, f\"{basename}_frequency_distribution.png\")\n        equalized_plot_path = os.path.join(OUTPUT_DIR, f\"equalized_{basename}_frequency_distribution.png\")\n\n        plot_histogram(encrypted_hist, f\"Encrypted Audio ({basename}) Frequency Distribution\", plot_path)\n        plot_histogram(encrypted_hist_equalized, f\"Equalized Encrypted Audio ({basename}) Frequency Distribution\", equalized_plot_path)\n\n    print(f\"Processing results saved to {OUTPUT_DIR}\")\n\nif __name__ == \"__main__\":\n    main()",
    "heatmap_compare.py": "import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import exposure\nimport wave\n\n# Constants\nRAW_AUDIO_PATH = \"./left.raw\"\nENCRYPTED_WAVS_DIR = \"./modified_raw\"\nOUTPUT_DIR = \"./adaptive_histogram_results\"\n\ndef read_raw_audio(file_path):\n    \"\"\"Read raw audio file as a NumPy array.\"\"\"\n    with open(file_path, \"rb\") as f:\n        return np.frombuffer(f.read(), dtype=np.uint8)\n\ndef read_wav_audio(file_path):\n    \"\"\"Read WAV file's audio data as a NumPy array of uint8.\"\"\"\n    with wave.open(file_path, 'rb') as wav_file:\n        num_frames = wav_file.getnframes()\n        audio_bytes = wav_file.readframes(num_frames)\n        return np.frombuffer(audio_bytes, dtype=np.uint8)\n\ndef compute_histogram(data):\n    \"\"\"Compute frequency histogram of byte data (0-255).\"\"\"\n    return np.bincount(data, minlength=256)\n\ndef apply_adaptive_histogram_equalization(hist, clip_limit=0.03):\n    \"\"\"Apply adaptive histogram equalization to the given histogram.\"\"\"\n    hist_normalized = hist.astype(np.float32) / hist.sum()\n    hist_equalized = exposure.equalize_adapthist(hist_normalized.reshape(16, 16), clip_limit=clip_limit).flatten()\n    hist_equalized = (hist_equalized * hist.sum()).astype(np.int32)\n    return hist_equalized\n\ndef plot_histogram(hist, title, output_path):\n    \"\"\"Plot the given histogram.\"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.bar(range(256), hist, color=\"blue\", alpha=0.7)\n    plt.title(title)\n    plt.xlabel(\"Byte Value\")\n    plt.ylabel(\"Frequency\")\n    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n    plt.savefig(output_path)\n    plt.close()\n\ndef main():\n    # Create output directory\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    # Read raw audio data\n    raw_data = read_raw_audio(RAW_AUDIO_PATH)\n    raw_hist = compute_histogram(raw_data)\n    raw_hist_equalized = apply_adaptive_histogram_equalization(raw_hist)\n\n    # Plot raw audio histograms\n    plot_histogram(raw_hist, \"Raw Audio Frequency Distribution\", \n                   os.path.join(OUTPUT_DIR, \"raw_frequency_distribution.png\"))\n    plot_histogram(raw_hist_equalized, \"Equalized Raw Audio Frequency Distribution\", \n                   os.path.join(OUTPUT_DIR, \"equalized_raw_frequency_distribution.png\"))\n\n    # Process each encrypted WAV file\n    encrypted_wavs_dir = ENCRYPTED_WAVS_DIR\n    for filename in os.listdir(encrypted_wavs_dir):\n        if filename.endswith(\".wav\"):\n            filepath = os.path.join(encrypted_wavs_dir, filename)\n            encrypted_data = read_wav_audio(filepath)\n            encrypted_hist = compute_histogram(encrypted_data)\n            encrypted_hist_equalized = apply_adaptive_histogram_equalization(encrypted_hist)\n\n            # Generate plot filenames with base name\n            basename = os.path.splitext(filename)[0]\n            plot_path = os.path.join(OUTPUT_DIR, f\"{basename}_frequency_distribution.png\")\n            equalized_plot_path = os.path.join(OUTPUT_DIR, f\"equalized_{basename}_frequency_distribution.png\")\n\n            plot_histogram(encrypted_hist, f\"Encrypted Audio ({basename}) Frequency Distribution\", plot_path)\n            plot_histogram(encrypted_hist_equalized, f\"Equalized Encrypted Audio ({basename}) Frequency Distribution\", equalized_plot_path)\n\n    print(f\"Processing results saved to {OUTPUT_DIR}\")\n\nif __name__ == \"__main__\":\n    main()",
    "extract_audio_features.py": "import librosa\nimport numpy as np\nimport os\n\ndef extract_audio_features(audio_file, sr=22050):\n    \"\"\"\n    Extracts MFCCs, Chroma features, Spectral Centroid, and Spectral Bandwidth from an audio file.\n\n    Args:\n        audio_file (str): Path to the audio file.\n        sr (int): Sample rate of the audio file.\n\n    Returns:\n        dict: A dictionary containing the extracted features.\n    \"\"\"\n    try:\n        y, _ = librosa.load(audio_file, sr=sr)\n    except Exception as e:\n        print(f\"Error loading audio file {audio_file}: {e}\")\n        return None\n\n    features = {}\n\n    # MFCCs\n    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)   # Adjust n_mfcc as needed\n    features['mfccs'] = np.mean(mfccs.T, axis=0)  # Average MFCCs over time\n\n    # Chroma features\n    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n    features['chroma'] = np.mean(chroma.T, axis=0)  # Average chroma over time\n\n    # Spectral Centroid\n    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr) \n    features['spectral_centroid'] = np.mean(spectral_centroid)\n\n    # Spectral Bandwidth\n    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n    features['spectral_bandwidth'] = np.mean(spectral_bandwidth)\n\n    return features",
    "brut-force-xor.py": "import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import exposure\nimport random\nimport datetime\nfrom handle_raw_audio import read_raw_audio\n\n# Constants\nRAW_AUDIO_PATH = \"./left.raw\"\nENCRYPTED_RAW_DIR = \"./modified_raw\"\nOUTPUT_DIR = \"./adaptive_histogram_results\"\nBIT_FLIP_OUTPUT_DIR = \"./bit_flip_modified\"\nNUM_BIT_FLIP_FILES = 100\nMIN_BIT_FLIP_PERCENTAGE = 0.001\nMAX_BIT_FLIP_PERCENTAGE = 0.1\n\n\ndef flip_bits_in_file(input_file, output_file, flip_percentage=0.01):\n    \"\"\"Reads a raw audio file, flips bits randomly, and saves the modified data.\"\"\"\n    with open(input_file, 'rb') as f_in:\n        data = bytearray(f_in.read())\n\n    num_bytes_to_flip = int(len(data) * flip_percentage)\n\n    for _ in range(num_bytes_to_flip):\n        byte_index = random.randint(0, len(data) - 1)\n        bit_index = random.randint(0, 7)  # 8 bits in a byte\n\n        # Flip the bit\n        data[byte_index] ^= (1 << bit_index)\n\n    with open(output_file, 'wb') as f_out:\n        f_out.write(data)\n\n\ndef generate_random_flipped_files(input_file, output_dir, num_files, min_percentage, max_percentage):\n    \"\"\"Generates multiple versions of the input file with random bit flips, using a random percentage within a given range.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    base_filename = os.path.splitext(os.path.basename(input_file))[0]\n\n    for i in range(num_files):\n        percentage = random.uniform(min_percentage, max_percentage)  # Generate random percentage\n        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        output_filename = f\"{base_filename}_flipped_{percentage:.4f}_{i+1:03d}_{timestamp}.raw\"\n        output_file = os.path.join(output_dir, output_filename)\n        flip_bits_in_file(input_file, output_file, percentage)\n        print(f\"Generated: {output_file}\")\n\n\ndef analyze_bit_flipped_files(bit_flip_output_dir, output_dir):\n    \"\"\"Analyze all bit-flipped files in the given directory.\"\"\"\n    for filename in os.listdir(bit_flip_output_dir):\n        if filename.endswith(\".raw\"):\n            filepath = os.path.join(bit_flip_output_dir, filename)\n            flipped_data = read_raw_audio(filepath)\n            flipped_hist = compute_histogram(flipped_data)\n            flipped_hist_equalized = apply_adaptive_histogram_equalization(flipped_hist)\n\n            basename = os.path.splitext(filename)[0]\n            plot_path = os.path.join(output_dir, f\"{basename}_frequency_distribution.png\")\n            equalized_plot_path = os.path.join(\n                output_dir, f\"equalized_{basename}_frequency_distribution.png\"\n            )\n\n            plot_histogram(flipped_hist, f\"Bit-Flipped ({basename}) Frequency Distribution\", plot_path)\n            plot_histogram(\n                flipped_hist_equalized, f\"Equalized Bit-Flipped ({basename}) Frequency Distribution\", equalized_plot_path\n            )\n\n\ndef compute_histogram(data):\n    # Tentative fix: Convert float32 data to integers in 0-255 range (APPROXIMATION - review logic!)\n    # Scale float data from -1 to 1 to 0 to 255 range (rough approximation)\n    int_data = ((data + 1.0) * 127.5).astype(int)  # Scale and shift to 0-255, then convert to int\n    # Clip values to ensure they are within 0-255 (just in case of rounding issues)\n    int_data = np.clip(int_data, 0, 255)\n    return np.bincount(int_data, minlength=256)\n\n\ndef apply_adaptive_histogram_equalization(hist, clip_limit=0.03):\n    \"\"\"Apply adaptive histogram equalization to the given histogram.\"\"\"\n    hist_normalized = hist.astype(np.float32) / hist.sum()\n    hist_equalized = exposure.equalize_adapthist(hist_normalized.reshape(16, 16), clip_limit=clip_limit).flatten()\n    hist_equalized = (hist_equalized * hist.sum()).astype(np.int32)\n    return hist_equalized\n\n\ndef plot_histogram(hist, title, output_path):\n    \"\"\"Plot the given histogram.\"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.bar(range(256), hist, color=\"blue\", alpha=0.7)\n    plt.title(title)\n    plt.xlabel(\"Byte Value\")\n    plt.ylabel(\"Frequency\")\n    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n    plt.savefig(output_path)\n    plt.close()\n\n\ndef calculate_chi_squared(hist1, hist2):\n    \"\"\"Compute Chi-squared distance between two histograms.\"\"\"\n    smooth = 1e-12\n    return np.sum((hist1 - hist2) ** 2 / (hist2 + smooth))\n\n\ndef brute_force_xor_decrypt(raw_data_path, encrypted_data_path, output_dir):\n    \"\"\"Attempt to decrypt using single-byte XOR brute force.\"\"\"\n    try:\n        with open(raw_data_path, 'rb') as f:\n            raw_data = np.frombuffer(f.read(), dtype=np.uint8)\n        with open(encrypted_data_path, 'rb') as f:\n            encrypted_data = np.frombuffer(f.read(), dtype=np.uint8)\n\n        min_length = min(len(raw_data), len(encrypted_data))\n        raw_data = raw_data[:min_length]\n        encrypted_data = encrypted_data[:min_length]\n\n        raw_hist = compute_histogram(raw_data)\n\n        best_key = None\n        best_distance = float('inf')\n\n        for key in range(256):\n            decrypted_data = np.bitwise_xor(encrypted_data, key)\n            decrypted_hist = compute_histogram(decrypted_data)\n            distance = calculate_chi_squared(decrypted_hist, raw_hist)\n\n            if distance < best_distance:\n                best_distance = distance\n                best_key = key\n\n        print(f\"\\nBest XOR Key: 0x{best_key:02X} (Chi\u00b2 Distance: {best_distance:.4f})\")\n\n        decrypted = np.bitwise_xor(encrypted_data, best_key)\n        output_file = os.path.join(\n            output_dir,\n            f\"decrypted_{os.path.basename(raw_data_path)}_{os.path.basename(encrypted_data_path)}.raw\",\n        )\n        with open(output_file, \"wb\") as f:\n            f.write(decrypted.tobytes())\n\n        print(f\"Decrypted data saved to '{output_file}'\")\n        return output_file\n\n    except FileNotFoundError as e:\n        print(f\"File not found: {e}\")\n        return None\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return None\n\n\ndef main():\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    os.makedirs(BIT_FLIP_OUTPUT_DIR, exist_ok=True)\n\n    # Read raw audio data\n    raw_data = read_raw_audio(RAW_AUDIO_PATH)\n    raw_hist = compute_histogram(raw_data)\n    raw_hist_equalized = apply_adaptive_histogram_equalization(raw_hist)\n\n    # Plot raw audio histograms\n    plot_histogram(\n        raw_hist,\n        \"Raw Audio Frequency Distribution\",\n        os.path.join(OUTPUT_DIR, \"raw_frequency_distribution.png\"),\n    )\n    plot_histogram(\n        raw_hist_equalized,\n        \"Equalized Raw Audio Frequency Distribution\",\n        os.path.join(OUTPUT_DIR, \"equalized_raw_frequency_distribution.png\"),\n    )\n\n    # Generate and analyze bit-flipped files\n    generate_random_flipped_files(RAW_AUDIO_PATH, BIT_FLIP_OUTPUT_DIR, NUM_BIT_FLIP_FILES, MIN_BIT_FLIP_PERCENTAGE, MAX_BIT_FLIP_PERCENTAGE)\n    analyze_bit_flipped_files(BIT_FLIP_OUTPUT_DIR, OUTPUT_DIR)\n\n    # Process each encrypted RAW file\n    encrypted_raw_dir = ENCRYPTED_RAW_DIR\n    for filename in os.listdir(encrypted_raw_dir):\n        if filename.endswith(\".raw\"):\n            filepath = os.path.join(encrypted_raw_dir, filename)\n            encrypted_data = read_raw_audio(filepath)\n            encrypted_hist = compute_histogram(encrypted_data)\n            encrypted_hist_equalized = apply_adaptive_histogram_equalization(encrypted_hist)\n\n            basename = os.path.splitext(filename)[0]\n            plot_path = os.path.join(OUTPUT_DIR, f\"{basename}_frequency_distribution.png\")\n            equalized_plot_path = os.path.join(OUTPUT_DIR, f\"equalized_{basename}_frequency_distribution.png\")\n\n            plot_histogram(\n                encrypted_hist,\n                f\"Encrypted Audio ({basename}) Frequency Distribution\",\n                plot_path,\n            )\n            plot_histogram(\n                encrypted_hist_equalized,\n                f\"Equalized Encrypted Audio ({basename}) Frequency Distribution\",\n                equalized_plot_path,\n            )\n\n            # Brute-force XOR decryption and analysis\n            decrypted_file = brute_force_xor_decrypt(RAW_AUDIO_PATH, filepath, OUTPUT_DIR)\n            if decrypted_file:\n                decrypted_data = read_raw_audio(decrypted_file)\n                decrypted_hist = compute_histogram(decrypted_data)\n                decrypted_hist_equalized = apply_adaptive_histogram_equalization(decrypted_hist)\n\n                basename = os.path.splitext(os.path.basename(decrypted_file))[0]\n                plot_path = os.path.join(OUTPUT_DIR, f\"{basename}_frequency_distribution.png\")\n                equalized_plot_path = os.path.join(\n                    OUTPUT_DIR, f\"equalized_{basename}_frequency_distribution.png\"\n                )\n\n                plot_histogram(\n                    decrypted_hist,\n                    f\"Decrypted ({basename}) Frequency Distribution\",\n                    plot_path,\n                )\n                plot_histogram(\n                    decrypted_hist_equalized,\n                    f\"Equalized Decrypted ({basename}) Frequency Distribution\",\n                    equalized_plot_path,\n                )\n\n    print(f\"Processing results saved to {OUTPUT_DIR}\")\n\n\nif __name__ == \"__main__\":\n    main()",
    "make_report.py": "import os\nimport subprocess\nimport json\nfrom datetime import datetime\n\n# Constants\nREPORT_FILE = \"encryption_analysis_report.txt\"\nSCRIPTS_TO_RUN = [\n  #  \"adaptive_histogram_equalization.py\",\n    \"analies.py\",\n    \"analyzz.py\",\n    \"brut-force-xor.py\",\n    \"create_variable_raw.py\",\n    \"entropy_analysis.py\",\n    \"file_directory_structure.py\",\n    \"heatmap_compare.py\",\n    \"images_modified_raw.py\",\n    \"sliding_window.py\",\n    \"time_frequency.py\"\n]\n\ndef run_script(script_name):\n    \"\"\"Run a Python script using subprocess.\"\"\"\n    print(f\"Running {script_name}...\")\n    try:\n        result = subprocess.run([\"python\", script_name], check=True, capture_output=True, text=True)\n        return result.stdout\n    except subprocess.CalledProcessError as e:\n        print(f\"Error running {script_name}: {e}\")\n        return None\n\ndef generate_report():\n    \"\"\"Generate a report summarizing the findings from all scripts.\"\"\"\n    with open(REPORT_FILE, 'w') as report:\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        report.write(f\"Encryption Analysis Report - Generated on {timestamp}\\n\")\n        report.write(\"=\"*80 + \"\\n\\n\")\n\n        # Run each script and collect its output\n        for script in SCRIPTS_TO_RUN:\n            report.write(f\"Results from {script}:\\n\")\n            report.write(\"-\"*60 + \"\\n\")\n            output = run_script(script)\n            if output:\n                report.write(output)\n            else:\n                report.write(\"Script failed to run or produced no output.\\n\")\n            report.write(\"\\n\" + \"=\"*80 + \"\\n\\n\")\n\n        # Additional analysis can be added here\n        report.write(\"Summary of Findings:\\n\")\n        report.write(\"-\"*60 + \"\\n\")\n        report.write(\"\"\"\nBased on the analysis conducted by the scripts:\n\n1. **Transformation Patterns**: The transformation patterns between raw and encrypted data suggest possible methods such as XOR encryption, modular arithmetic, or byte mapping.\n2. **Entropy Analysis**: The entropy of the encrypted files is higher than the raw file, indicating effective obfuscation.\n3. **Histogram Equalization**: Adaptive histogram equalization shows clear differences between raw and encrypted distributions.\n4. **Brute Force XOR**: A brute force approach identified potential XOR keys that could decrypt the audio.\n5. **Decryption Validation**: Decrypted files show frequency distributions closer to the raw file, validating the decryption method.\n\nRecommendations:\n- Apply the most successful decryption method (likely XOR) to the larger dataset.\n- Further refine the decryption algorithm based on observed patterns.\n- Validate the decrypted audio by listening to restored drumbeats.\n\n\"\"\")\n        report.write(\"=\"*80 + \"\\n\")\n # Process all audio files in the output directory\n    audio_directory = \"./adaptive_histogram_results\"  # Adjust as needed\n    audio_features = process_audio_files(audio_directory)\n\n    # Add audio features to the report\n    report.write(\"Audio Feature Extraction Results:\\n\")\n    report.write(\"-\" * 60 + \"\\n\")\n    for filename, features in audio_features.items():\n        report.write(f\"{filename}:\\n\")\n        for feature_name, feature_value in features.items():\n            report.write(f\"  {feature_name}: {feature_value}\\n\")\n        report.write(\"\\n\")\nif __name__ == \"__main__\":\n    generate_report()\n    print(f\"Report generated: {REPORT_FILE}\")",
    "file_directory_structure.py": "import os\nimport json\n\ndef list_python_files(path):\n    \"\"\"\n    Lists all `.py` files in the given directory.\n    \n    Args:\n    path (str): The path to the directory.\n    \"\"\"\n    \n    python_files = {}\n    \n    # Iterate over each item in the directory\n    for item in os.listdir(path):\n        # Construct the full path to the item\n        item_path = os.path.join(path, item)\n        \n        # Check if the item is a Python file\n        if os.path.isfile(item_path) and item.endswith(\".py\"):\n            with open(item_path, \"r\") as file:\n                python_files[item] = file.read()\n    \n    return python_files\n\ndef main():\n    # Get the current working directory\n    current_dir = os.getcwd()\n    print(f\"Current Directory: {current_dir}\")\n    \n    # List the `.py` files in the current directory\n    python_files = list_python_files(current_dir)\n    \n    # Write the `.py` files to a JSON file\n    with open(\"python_files.json\", \"w\") as json_file:\n        json.dump(python_files, json_file, indent=4)\n\nif __name__ == \"__main__\":\n    main()",
    "bcatf_analyze.py": "import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport librosa\nimport librosa.display\nfrom scipy.signal import correlate, correlation_lags\nfrom scipy.fft import fft, ifft\nfrom skimage import exposure\nimport glob\n\n# Constants\nRAW_AUDIO_PATH = \"./left.raw\"\nENCRYPTED_RAW_DIR = \"./modified_raw\"\nOUTPUT_DIR = \"./comprehensive_analysis\"\n\ndef read_raw_audio(file_path):\n    \"\"\"Read raw audio file as a NumPy array.\"\"\"\n    with open(file_path, \"rb\") as f:\n        return np.frombuffer(f.read(), dtype=np.uint8)\n\ndef compute_histogram(data):\n    \"\"\"Compute frequency histogram of byte data (0-255).\"\"\"\n    return np.bincount(data, minlength=256)\n\ndef plot_histogram(hist, title, output_path):\n    \"\"\"Plot the given histogram with hex values on x-axis.\"\"\"\n    plt.figure(figsize=(12, 6))\n    plt.bar(range(256), hist, color=\"blue\", alpha=0.7)\n    plt.title(title)\n    \n    # Add hex labels at key positions\n    hex_positions = list(range(0, 256, 16))\n    hex_labels = [f\"{i:02X}\" for i in hex_positions]\n    plt.xticks(hex_positions, hex_labels)\n    \n    plt.xlabel(\"Byte Value (Hex)\")\n    plt.ylabel(\"Frequency\")\n    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n    plt.savefig(output_path)\n    plt.close()\n\ndef compute_bigram_frequencies(data):\n    \"\"\"Computes bigram frequency distribution.\"\"\"\n    bigrams = [tuple(data[i:i+2]) for i in range(len(data)-1)]\n    unique, counts = np.unique(bigrams, axis=0, return_counts=True)\n    freq_matrix = np.zeros((256, 256), dtype=np.float32)\n    \n    for (a, b), count in zip(unique, counts):\n        freq_matrix[a][b] = count / len(bigrams)  # Normalize to probability\n    \n    return freq_matrix\n\ndef plot_bigram_heatmap(matrix, title, output_path):\n    \"\"\"Plots a bigram frequency heatmap.\"\"\"\n    plt.figure(figsize=(10, 10))\n    plt.imshow(matrix, cmap='viridis', interpolation='nearest')\n    plt.colorbar(label='Probability')\n    plt.title(title)\n    plt.xlabel('Second Byte')\n    plt.ylabel('First Byte')\n    \n    # Add some hex labels\n    hex_positions = list(range(0, 256, 32))\n    hex_labels = [f\"{i:02X}\" for i in hex_positions]\n    plt.xticks(hex_positions, hex_labels)\n    plt.yticks(hex_positions, hex_labels)\n    \n    plt.savefig(output_path)\n    plt.close()\n\ndef compute_cross_correlation(data1, data2):\n    \"\"\"Compute cross-correlation between two datasets.\"\"\"\n    return correlate(data1, data2, mode='full')\n\ndef plot_cross_correlation(correlation, lags, title, output_path):\n    \"\"\"Plot cross-correlation.\"\"\"\n    plt.figure(figsize=(12, 6))\n    plt.plot(lags, correlation)\n    plt.title(title)\n    plt.xlabel('Lag')\n    plt.ylabel('Correlation')\n    plt.grid(True)\n    plt.savefig(output_path)\n    plt.close()\n\ndef compute_auto_correlation(data):\n    \"\"\"Compute auto-correlation of a dataset.\"\"\"\n    return correlate(data, data, mode='full')\n\ndef plot_auto_correlation(auto_corr, lags, title, output_path):\n    \"\"\"Plot auto-correlation.\"\"\"\n    plt.figure(figsize=(12, 6))\n    plt.plot(lags, auto_corr)\n    plt.title(title)\n    plt.xlabel('Lag')\n    plt.ylabel('Auto-Correlation')\n    plt.grid(True)\n    plt.savefig(output_path)\n    plt.close()\n\ndef compute_stft(data, n_fft=512, hop_length=256):\n    \"\"\"Compute Short-Time Fourier Transform (STFT).\"\"\"\n    return librosa.stft(data.astype(np.float32), n_fft=n_fft, hop_length=hop_length)\n\ndef plot_stft(stft_matrix, title, output_path):\n    \"\"\"Plot STFT magnitude spectrogram.\"\"\"\n    plt.figure(figsize=(12, 6))\n    librosa.display.specshow(librosa.amplitude_to_db(np.abs(stft_matrix), ref=np.max), sr=22050, x_axis='time', y_axis='log')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(title)\n    plt.tight_layout()\n    plt.savefig(output_path)\n    plt.close()\n\ndef extract_audio_features(audio_file):\n    \"\"\"Extracts MFCCs, Chroma features, Spectral Centroid, and Spectral Bandwidth from an audio file.\"\"\"\n    try:\n        y, sr = librosa.load(audio_file, sr=None)\n    except Exception as e:\n        print(f\"Error loading audio file {audio_file}: {e}\")\n        return None\n\n    features = {}\n\n    # MFCCs\n    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)   # Adjust n_mfcc as needed\n    features['mfccs'] = np.mean(mfccs.T, axis=0)  # Average MFCCs over time\n\n    # Chroma features\n    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n    features['chroma'] = np.mean(chroma.T, axis=0)  # Average chroma over time\n\n    # Spectral Centroid\n    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr) \n    features['spectral_centroid'] = np.mean(spectral_centroid)\n\n    # Spectral Bandwidth\n    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n    features['spectral_bandwidth'] = np.mean(spectral_bandwidth)\n\n    return features\n\ndef main():\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    # Read raw audio data\n    raw_data = read_raw_audio(RAW_AUDIO_PATH)\n    raw_hist = compute_histogram(raw_data)\n    plot_histogram(raw_hist, \"Raw Audio Frequency Distribution\", os.path.join(OUTPUT_DIR, \"raw_frequency_distribution.png\"))\n\n    # Process each encrypted RAW file\n    encrypted_files = glob.glob(os.path.join(ENCRYPTED_RAW_DIR, \"*.raw\"))\n    if not encrypted_files:\n        print(\"No encrypted files found!\")\n        return\n\n    for encrypted_file in encrypted_files:\n        basename = os.path.splitext(os.path.basename(encrypted_file))[0]\n        encrypted_data = read_raw_audio(encrypted_file)\n        encrypted_hist = compute_histogram(encrypted_data)\n        plot_histogram(encrypted_hist, f\"Encrypted Audio ({basename}) Frequency Distribution\", os.path.join(OUTPUT_DIR, f\"{basename}_frequency_distribution.png\"))\n\n        # Bigram Analysis\n        raw_bigram_freq = compute_bigram_frequencies(raw_data)\n        encrypted_bigram_freq = compute_bigram_frequencies(encrypted_data)\n        \n        plot_bigram_heatmap(raw_bigram_freq, \"Raw Audio Bigram Heatmap\", os.path.join(OUTPUT_DIR, f\"{basename}_raw_bigram_heatmap.png\"))\n        plot_bigram_heatmap(encrypted_bigram_freq, \"Encrypted Audio Bigram Heatmap\", os.path.join(OUTPUT_DIR, f\"{basename}_encrypted_bigram_heatmap.png\"))\n        \n        # Cross-Correlation\n        cross_corr = compute_cross_correlation(raw_data, encrypted_data)\n        lags = correlation_lags(len(raw_data), len(encrypted_data), mode='full')\n        plot_cross_correlation(cross_corr, lags, f\"Cross-Correlation (Raw vs Encrypted {basename})\", os.path.join(OUTPUT_DIR, f\"{basename}_cross_correlation.png\"))\n        \n        # Auto-Correlation\n        raw_auto_corr = compute_auto_correlation(raw_data)\n        encrypted_auto_corr = compute_auto_correlation(encrypted_data)\n        raw_lags = correlation_lags(len(raw_data), len(raw_data), mode='full')\n        encrypted_lags = correlation_lags(len(encrypted_data), len(encrypted_data), mode='full')\n        \n        plot_auto_correlation(raw_auto_corr, raw_lags, f\"Auto-Correlation (Raw {basename})\", os.path.join(OUTPUT_DIR, f\"{basename}_raw_auto_correlation.png\"))\n        plot_auto_correlation(encrypted_auto_corr, encrypted_lags, f\"Auto-Correlation (Encrypted {basename})\", os.path.join(OUTPUT_DIR, f\"{basename}_encrypted_auto_correlation.png\"))\n        \n        # Time-Frequency Analysis\n        raw_stft = compute_stft(raw_data)\n        encrypted_stft = compute_stft(encrypted_data)\n        \n        plot_stft(raw_stft, f\"STFT Magnitude Spectrogram (Raw {basename})\", os.path.join(OUTPUT_DIR, f\"{basename}_raw_stft.png\"))\n        plot_stft(encrypted_stft, f\"STFT Magnitude Spectrogram (Encrypted {basename})\", os.path.join(OUTPUT_DIR, f\"{basename}_encrypted_stft.png\"))\n        \n        # Feature Extraction\n        raw_features = extract_audio_features(RAW_AUDIO_PATH)\n        encrypted_features = extract_audio_features(encrypted_file)\n        \n        if raw_features:\n            print(f\"Raw Audio Features: {raw_features}\")\n        if encrypted_features:\n            print(f\"Encrypted Audio Features ({basename}): {encrypted_features}\")\n\n    print(f\"All results saved to {OUTPUT_DIR}\")\n\nif __name__ == \"__main__\":\n    main()",
    "idk.py": "import os\nimport numpy as np\nimport pandas as pd\nfrom itertools import product\nfrom scipy.stats import entropy\nfrom sklearn.preprocessing import StandardScaler\nimport glob\nfrom skimage import exposure\nimport matplotlib.pyplot as plt\n\n# Import custom functions from other scripts (assuming they are in the same directory)\nfrom hex_frequency_dist import compute_frequency_distribution\nfrom similarity import compute_mse, compute_ssi, compute_pearson\nfrom bit_flip_analysis import analyze_bit_flips\nfrom aggregate_scores import aggregate_scores\nfrom make_charts import analyze_score_files\n\n# Configuration Parameters\nRAW_AUDIO_PATH = \"./left.raw\"\nENCRYPTED_AUDIO_PATH = \"./right.raw\"\nDECRYPTED_AUDIO_DIR = \"./\"\nOUTPUT_DIR = \"./ml_data\"\nSCORE_TYPES = [\"absdiff\", \"chiSquared\", \"euclidean\", \"pearson\", \"spearman\"]\nENCRYPTED_RAW_DIR = \"./modified_raw\" #add encrypted raw directory for histogram data.\nHISTOGRAM_OUTPUT_DIR = \"./histogram_ml_data\" #add histogram output directory\n\ndef load_audio_data(filepath):\n    \"\"\"Load binary audio file as a NumPy array.\"\"\"\n    with open(filepath, \"rb\") as f:\n        return np.frombuffer(f.read(), dtype=np.uint8)\n\ndef compute_histogram(data):\n    \"\"\"Compute frequency histogram of byte data (0-255).\"\"\"\n    return np.bincount(data, minlength=256)\n\ndef apply_adaptive_histogram_equalization(hist, clip_limit=0.03):\n    \"\"\"Apply adaptive histogram equalization to the given histogram.\"\"\"\n    hist_normalized = hist.astype(np.float32) / hist.sum()\n    hist_equalized = exposure.equalize_adapthist(hist_normalized.reshape(16, 16), clip_limit=clip_limit).flatten()\n    hist_equalized = (hist_equalized * hist.sum()).astype(np.int32)\n    return hist_equalized\n\ndef extract_features(raw_data, decrypted_data, encrypted_data=None):\n    \"\"\"Extract features from raw, decrypted and optionally encrypted audio data.\"\"\"\n    # Frequency Distribution\n    raw_freq = compute_frequency_distribution(raw_data)\n    decrypted_freq = compute_frequency_distribution(decrypted_data)\n    \n    # Similarity Metrics\n    mse = compute_mse(raw_data, decrypted_data)\n    ssi = compute_ssi(raw_data, decrypted_data)\n    pearson_corr = compute_pearson(raw_data, decrypted_data)\n    \n    # Bit Flip Analysis\n    bit_flip_counts = analyze_bit_flips(raw_data, decrypted_data)\n    \n    # Entropy\n    raw_entropy = entropy(raw_freq + 1e-10)  # Add epsilon to avoid log(0)\n    decrypted_entropy = entropy(decrypted_freq + 1e-10)\n\n    # Histogram Features\n    if encrypted_data is not None:\n        encrypted_hist = compute_histogram(encrypted_data)\n        encrypted_hist_equalized = apply_adaptive_histogram_equalization(encrypted_hist)\n        encrypted_entropy = entropy(encrypted_hist+1e-10)\n        encrypted_equalized_entropy = entropy(encrypted_hist_equalized+1e-10)\n\n        features = {\n            \"mse\": mse,\n            \"ssi\": ssi,\n            \"pearson_corr\": pearson_corr,\n            \"raw_entropy\": raw_entropy,\n            \"decrypted_entropy\": decrypted_entropy,\n            \"encrypted_entropy\": encrypted_entropy,\n            \"encrypted_equalized_entropy\": encrypted_equalized_entropy,\n            **{f\"bit_flip_{i}\": count for i, count in enumerate(bit_flip_counts)},\n            **{f\"encrypted_hist_{i}\": val for i, val in enumerate(encrypted_hist)},\n            **{f\"encrypted_eq_hist_{i}\": val for i, val in enumerate(encrypted_hist_equalized)},\n        }\n    else:\n        features = {\n            \"mse\": mse,\n            \"ssi\": ssi,\n            \"pearson_corr\": pearson_corr,\n            \"raw_entropy\": raw_entropy,\n            \"decrypted_entropy\": decrypted_entropy,\n            **{f\"bit_flip_{i}\": count for i, count in enumerate(bit_flip_counts)},\n        }\n\n    return features\n\ndef process_decryption_results():\n    \"\"\"Process all decrypted audio files and extract features.\"\"\"\n    raw_data = load_audio_data(RAW_AUDIO_PATH)\n    results = []\n    \n    for filename in os.listdir(DECRYPTED_AUDIO_DIR):\n        if filename.startswith(\"decrypted_audio_\") and filename.endswith(\".wav\"):\n            print(f\"Processing: {filename}\")\n            decrypted_data = load_audio_data(os.path.join(DECRYPTED_AUDIO_DIR, filename))\n\n            encrypted_filename = filename.replace(\"decrypted_audio_\", \"\").replace(\".wav\", \".raw\")\n            encrypted_filepath = os.path.join(ENCRYPTED_RAW_DIR, encrypted_filename)\n\n            if os.path.exists(encrypted_filepath):\n                encrypted_data = load_audio_data(encrypted_filepath)\n                features = extract_features(raw_data, decrypted_data, encrypted_data)\n            else:\n                features = extract_features(raw_data, decrypted_data)\n\n            features[\"filename\"] = filename\n            \n            # Add Decryption Scores\n            scores = aggregate_scores(DECRYPTED_AUDIO_DIR)\n            if filename in scores:\n                features.update(scores[filename])\n            \n            results.append(features)\n    \n    return results\n\ndef save_to_csv(data, output_path):\n    \"\"\"Save extracted features to a CSV file.\"\"\"\n    df = pd.DataFrame(data)\n    \n    # Normalize numerical features\n    numerical_cols = df.select_dtypes(include=[np.number]).columns\n    scaler = StandardScaler()\n    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n    \n    df.to_csv(output_path, index=False)\n    print(f\"Data saved to {output_path}\")\n\ndef main():\n    # Create output directory\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    os.makedirs(HISTOGRAM_OUTPUT_DIR, exist_ok=True)\n    \n    # Process decryption results and extract features\n    print(\"Processing decryption results...\")\n    ml_data = process_decryption_results()\n    \n    # Save processed data to CSV\n    output_file = os.path.join(OUTPUT_DIR, \"ml_training_data.csv\")\n    save_to_csv(ml_data, output_file)\n    \n    print(\"Machine learning data generation complete.\")\n\nif __name__ == \"__main__\":\n    main()",
    "analies.py": "# analies.py\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import entropy\nimport glob\nfrom handle_raw_audio import read_raw_audio\n\n# Constants\nRAW_AUDIO_PATH = \"./left.raw\"\nENCRYPTED_RAW_DIR = \"./modified_raw\"\nOUTPUT_DIR = \"./decryption_analysis\"\n\nimport numpy as np\n\ndef compute_histogram(data):\n    # Tentative fix: Convert float32 data to integers in 0-255 range (APPROXIMATION - review logic!)\n    # Scale float data from -1 to 1 to 0 to 255 range (rough approximation)\n    int_data = ((data + 1.0) * 127.5).astype(int)  # Scale and shift to 0-255, then convert to int\n    # Clip values to ensure they are within 0-255 (just in case of rounding issues)\n    int_data = np.clip(int_data, 0, 255)\n    return np.bincount(int_data, minlength=256)\n\ndef plot_histogram(hist, title, output_path):\n    \"\"\"Plot the given histogram with hex values on x-axis.\"\"\"\n    plt.figure(figsize=(12, 6))\n    plt.bar(range(256), hist, color=\"blue\", alpha=0.7)\n    plt.title(title)\n    \n    # Add hex labels at key positions\n    hex_positions = list(range(0, 256, 16))\n    hex_labels = [f\"{i:02X}\" for i in hex_positions]\n    plt.xticks(hex_positions, hex_labels)\n    \n    plt.xlabel(\"Byte Value (Hex)\")\n    plt.ylabel(\"Frequency\")\n    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n    plt.savefig(output_path)\n    plt.close()\n\ndef analyze_transformation_patterns(raw_data, encrypted_data):\n    \"\"\"Analyze possible transformation patterns between raw and encrypted data.\"\"\"\n    # Create a transformation mapping\n    transform_mapping = {}\n    inverse_mapping = {}\n    \n    # Find byte pairs that occur most frequently\n    min_length = min(len(raw_data), len(encrypted_data))\n    for i in range(min_length):\n        raw_byte = raw_data[i]\n        enc_byte = encrypted_data[i]\n        \n        if raw_byte not in transform_mapping:\n            transform_mapping[raw_byte] = {}\n        \n        if enc_byte not in transform_mapping[raw_byte]:\n            transform_mapping[raw_byte][enc_byte] = 0\n        \n        transform_mapping[raw_byte][enc_byte] += 1\n        \n        # Also track inverse mapping\n        if enc_byte not in inverse_mapping:\n            inverse_mapping[enc_byte] = {}\n        \n        if raw_byte not in inverse_mapping[enc_byte]:\n            inverse_mapping[enc_byte][raw_byte] = 0\n        \n        inverse_mapping[enc_byte][raw_byte] += 1\n    \n    # Determine most likely transformation rule\n    transformation_rule = {}\n    inverse_rule = {}\n    \n    for raw_byte, mappings in transform_mapping.items():\n        most_common_enc = max(mappings.items(), key=lambda x: x[1])[0]\n        transformation_rule[raw_byte] = most_common_enc\n    \n    for enc_byte, mappings in inverse_mapping.items():\n        most_common_raw = max(mappings.items(), key=lambda x: x[1])[0]\n        inverse_rule[enc_byte] = most_common_raw\n    \n    return transformation_rule, inverse_rule\n\ndef analyze_xor_key(raw_data, encrypted_data):\n    print(\"Analyzing XOR key...\")\n    possible_keys = {}\n\n    # Convert to integers\n    raw_data_int = ((raw_data + 1.0) * 127.5).astype(int)\n    encrypted_data_int = ((encrypted_data + 1.0) * 127.5).astype(int)\n\n    for i in range(min(len(raw_data_int), len(encrypted_data_int))):\n        key = raw_data_int[i] ^ encrypted_data_int[i]\n        key = np.clip(key, 0, 255)\n        if key not in possible_keys:\n            possible_keys[key] = 0\n        possible_keys[key] += 1\n\n    # Convert dictionary to list of (key, frequency) tuples and sort by frequency (descending)\n    sorted_keys = sorted(possible_keys.items(), key=lambda item: item[1], reverse=True) # Sort by value (frequency)\n\n    return sorted_keys # Return the sorted list\n\ndef test_byte_mapping(raw_data, encrypted_data, output_dir):\n    \"\"\"Test if there's a consistent byte-to-byte mapping.\"\"\"\n    min_length = min(len(raw_data), len(encrypted_data))\n    mapping = np.zeros((256, 256), dtype=np.int32)\n    \n    for i in range(min_length):\n        mapping[raw_data[i], encrypted_data[i]] += 1\n    \n    # Plot the mapping as a heatmap\n    plt.figure(figsize=(10, 10))\n    plt.imshow(np.log1p(mapping), cmap='viridis')\n    plt.colorbar(label='log(count + 1)')\n    plt.title('Byte Mapping Heatmap (Raw \u2192 Encrypted)')\n    plt.xlabel('Encrypted Byte')\n    plt.ylabel('Raw Byte')\n    \n    # Add some hex labels\n    hex_positions = list(range(0, 256, 32))\n    hex_labels = [f\"{i:02X}\" for i in hex_positions]\n    plt.xticks(hex_positions, hex_labels)\n    plt.yticks(hex_positions, hex_labels)\n    \n    plt.savefig(os.path.join(output_dir, \"byte_mapping_heatmap.png\"))\n    plt.close()\n    \n    # Find the most common mapping for each raw byte\n    forward_mapping = {}\n    for i in range(256):\n        if np.sum(mapping[i, :]) > 0:\n            forward_mapping[i] = np.argmax(mapping[i, :])\n    \n    return forward_mapping\n\ndef decrypt_with_mapping(encrypted_data, mapping):\n    \"\"\"Decrypt data using a byte-to-byte mapping.\"\"\"\n    decrypted = np.zeros_like(encrypted_data)\n    for i in range(len(encrypted_data)):\n        if encrypted_data[i] in mapping:\n            decrypted[i] = mapping[encrypted_data[i]]\n        else:\n            decrypted[i] = encrypted_data[i]  # Keep unchanged if no mapping\n    return decrypted\n\ndef analyze_modular_arithmetic(raw_data, encrypted_data):\n    \"\"\"Test if the transformation follows the pattern: enc = (raw + key) % 256 or enc = (raw - key) % 256.\"\"\"\n    min_length = min(len(raw_data), len(encrypted_data))\n    \n    # Cast to int32 to avoid overflow during subtraction\n    raw_data_int32 = raw_data.astype(np.int32)\n    encrypted_data_int32 = encrypted_data.astype(np.int32)\n    \n    # Test addition\n    add_keys = {}\n    for i in range(min_length): \n        key = (encrypted_data_int32[i] - raw_data_int32[i]) % 256\n        if key not in add_keys:\n            add_keys[key] = 0\n        add_keys[key] += 1\n    \n    # Test subtraction\n    sub_keys = {}\n    for i in range(min_length):\n        key = (raw_data_int32[i] - encrypted_data_int32[i]) % 256\n        if key not in sub_keys:\n            sub_keys[key] = 0\n        sub_keys[key] += 1\n    \n    # Get top candidates\n    top_add_keys = sorted(add_keys.items(), key=lambda x: x[1], reverse=True)[:3]\n    top_sub_keys = sorted(sub_keys.items(), key=lambda x: x[1], reverse=True)[:3]\n    \n    return {\n        \"addition\": top_add_keys,\n        \"subtraction\": top_sub_keys\n    }\n\ndef test_decryption_methods(raw_data, encrypted_data, OUTPUT_DIR):\n    print(\"\\nTesting possible decryption methods...\")\n    xor_results = analyze_xor_key(raw_data, encrypted_data)\n    if not xor_results:  # Check if xor_results is empty\n        print(\"No XOR keys found to analyze.\")\n        return {} # Return empty dictionary if no keys\n\n    best_xor_key_tuple = xor_results[0] # Get the tuple (key, frequency) for the most frequent key\n    best_xor_key = best_xor_key_tuple[0] # Extract just the key (the first element of the tuple)\n    best_xor_similarity = xor_results[0][1] # Extract the frequency (second element of the tuple)\n\n    print(f\"Best XOR key: {best_xor_key}, Frequency: {best_xor_similarity}\")\n\n    # Convert encrypted_data to integer format BEFORE bitwise_xor\n    encrypted_data_int = ((encrypted_data + 1.0) * 127.5).astype(int)\n\n    # Apply XOR decryption with the best key\n    decrypted_xor = np.bitwise_xor(encrypted_data_int, best_xor_key)  # Use the INT version of encrypted_data\n\n    # Clip and convert decrypted data back to float for saving (if needed for audio output)\n    decrypted_xor_float = np.clip(decrypted_xor / 127.5 - 1.0, -1.0, 1.0).astype(np.float32)  # Reverse scaling and clip\n\n    output_file_xor = os.path.join(OUTPUT_DIR, \"decrypted_xor.raw\")\n    sf.write(output_file_xor, decrypted_xor_float, 16000, format='RAW', subtype='PCM_16')\n    print(f\"Decrypted XOR audio saved to: {output_file_xor}\")\n\n    frequency_analysis_results = analyze_frequency(decrypted_xor_float, raw_hist)  # Pass float data for frequency analysis if it expects it\n\n    # Placeholder for other decryption methods - you can add more decryption tests here\n    # For example, Caesar cipher, etc., and analyze them similarly\n\n    return {\"xor\": (best_xor_key, best_xor_similarity, output_file_xor, frequency_analysis_results)}\ndef analyze_frequency(data, raw_hist):\n    print(\"Placeholder: analyze_frequency function called, but not yet implemented.\")\n    return None  # Or return some default value\n\ndef main():\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    \n    # Read raw audio data\n    raw_data = read_raw_audio(RAW_AUDIO_PATH)\n    raw_hist = compute_histogram(raw_data)\n    plot_histogram(raw_hist, \"Raw Audio Frequency Distribution\", \n                 os.path.join(OUTPUT_DIR, \"raw_frequency_distribution.png\"))\n    \n    # Process the first encrypted file for initial analysis\n    encrypted_files = glob.glob(os.path.join(ENCRYPTED_RAW_DIR, \"*.raw\"))\n    if not encrypted_files:\n        print(\"No encrypted files found!\")\n        return\n    \n    first_file = encrypted_files[0]\n    print(f\"Analyzing primary file: {first_file}\")\n    encrypted_data = read_raw_audio(first_file)\n    encrypted_hist = compute_histogram(encrypted_data)\n    plot_histogram(encrypted_hist, f\"Encrypted Audio Frequency Distribution\", \n                 os.path.join(OUTPUT_DIR, \"encrypted_frequency_distribution.png\"))\n    \n    # Analyze transformation patterns\n    print(\"Testing possible decryption methods...\")\n    decryption_results = test_decryption_methods(raw_data, encrypted_data, OUTPUT_DIR)\n    \n    # Report results\n    print(\"\\nDecryption Method Results (Sorted by Similarity):\")\n    for method, key, similarity, _ in decryption_results:\n        print(f\"Method: {method}, Key: {key}, Similarity: {similarity:.4f}\")\n    \n    best_method, best_key, best_similarity, best_decrypted = decryption_results[0]\n    print(f\"\\nBest method: {best_method} with key {best_key} (Similarity: {best_similarity:.4f})\")\n    \n    # Apply the best method to all encrypted files\n    print(\"\\nApplying best decryption method to all encrypted files...\")\n    for encrypted_file in encrypted_files:\n        basename = os.path.splitext(os.path.basename(encrypted_file))[0]\n        encrypted_data = read_raw_audio(encrypted_file)\n        \n        # Apply the best decryption method\n        if best_method == \"XOR\":\n            decrypted = np.bitwise_xor(encrypted_data, best_key)\n        elif best_method == \"Addition\":\n            decrypted = (encrypted_data - best_key) % 256\n        elif best_method == \"Subtraction\":\n            decrypted = (encrypted_data + best_key) % 256\n        elif best_method == \"Mapping\":\n            # Recreate the inverse mapping for the best result\n            forward_mapping = test_byte_mapping(raw_data, read_raw_audio(first_file), OUTPUT_DIR)\n            inverse_mapping = {v: k for k, v in forward_mapping.items()}\n            decrypted = decrypt_with_mapping(encrypted_data, inverse_mapping)\n        \n        # Save the decrypted file\n        output_file = os.path.join(OUTPUT_DIR, f\"decrypted_{basename}.raw\")\n        with open(output_file, \"wb\") as f:\n            f.write(decrypted.tobytes())\n        \n        print(f\"Decrypted {basename} saved to {output_file}\")\n        \n        # Extract features from decrypted audio\n        decrypted_features = extract_audio_features(output_file)\n        if decrypted_features:\n            print(f\"Decrypted Audio Features ({basename}): {decrypted_features}\")\n    \n    print(f\"\\nAll results saved to {OUTPUT_DIR}\")\n\nif __name__ == \"__main__\":\n    main()",
    "time_frequency.py": "import numpy as np\nimport matplotlib.pyplot as plt\nimport glob\nimport os\n\ndef read_binary_file(file_path):\n    \"\"\"Reads a binary file and returns its content as a numpy array of bytes\"\"\"\n    with open(file_path, 'rb') as f:\n        return np.frombuffer(f.read(), dtype=np.uint8)\n\ndef compute_bigram_frequencies(data):\n    \"\"\"Computes bigram frequency distribution\"\"\"\n    bigrams = [tuple(data[i:i+2]) for i in range(len(data)-1)]\n    unique, counts = np.unique(bigrams, axis=0, return_counts=True)\n    freq_matrix = np.zeros((256, 256), dtype=np.float32)\n    \n    for (a, b), count in zip(unique, counts):\n        freq_matrix[a][b] = count / len(bigrams)  # Normalize to probability\n    \n    return freq_matrix\n\ndef plot_heatmap(matrix, title, ax, cmap='viridis'):\n    \"\"\"Plots a heatmap on the given axes\"\"\"\n    im = ax.imshow(matrix, cmap=cmap, interpolation='nearest', vmin=0, vmax=np.max(matrix))\n    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n    ax.set_title(title)\n    ax.set_xlabel('Second Byte')\n    ax.set_ylabel('First Byte')\n    ax.set_xticks(np.arange(0, 256, 32))\n    ax.set_yticks(np.arange(0, 256, 32))\n\ndef compare_heatmaps(raw_path, encrypted_path, output_filename='heatmap_comparison.png'):\n    \"\"\"Generates and saves heatmaps comparing raw and encrypted files\"\"\"\n    # Read and truncate data to same length\n    raw_data = read_binary_file(raw_path)\n    encrypted_data = read_binary_file(encrypted_path)\n    min_len = min(len(raw_data), len(encrypted_data))\n    raw_data = raw_data[:min_len]\n    encrypted_data = encrypted_data[:min_len]\n\n    # Compute bigram frequencies\n    raw_freq = compute_bigram_frequencies(raw_data)\n    encrypted_freq = compute_bigram_frequencies(encrypted_data)\n\n    # Create comparison plot\n    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n    # Plot individual heatmaps\n    plot_heatmap(raw_freq, 'Raw Audio Bigram Heatmap', axes[0])\n    plot_heatmap(encrypted_freq, 'Encrypted Audio Bigram Heatmap', axes[1])\n\n    # Plot difference heatmap\n    diff = np.abs(raw_freq - encrypted_freq)\n    plot_heatmap(diff, 'Difference Heatmap', axes[2], cmap='plasma')\n\n    plt.tight_layout()\n    plt.savefig(output_filename)\n    plt.close(fig)  # Close the figure to free up memory\n\nif __name__ == \"__main__\":\n    raw_path = 'left.raw'\n    modified_folder = './modified_raw'\n    encrypted_files = glob.glob(os.path.join(modified_folder, '*.raw'))\n    \n    # Create output directory if it doesn't exist\n    output_directory = './time_freq_png'\n    os.makedirs(output_directory, exist_ok=True)\n    \n    for encrypted_file in encrypted_files:\n        base_name = os.path.splitext(os.path.basename(encrypted_file))[0]\n        output_filename = os.path.join(output_directory, f\"{base_name}_comparison.png\")\n        compare_heatmaps(raw_path, encrypted_file, output_filename)",
    "handle_raw_audio.py": "import librosa\nimport soundfile as sf\nimport numpy as np\n\ndef read_raw_audio(file_path, sr=16000, channels=1): # Added channels parameter, default to 1 (mono)\n    \"\"\"Reads raw audio data and resamples it.\"\"\"\n    try:\n        # Provide samplerate and channels to sf.read for raw files\n        data, samplerate = sf.read(file_path, samplerate=sr, channels=channels, format='RAW', subtype='PCM_16') # Explicitly define format as RAW and subtype\n    except Exception as e:\n        raise Exception(f\"Error reading audio file {file_path}: {e}\")\n\n    if len(data.shape) > 1:\n        data = np.mean(data, axis=1)  # Convert to mono if stereo (though should be mono already if channels=1 above)\n\n    # Convert to floating-point (e.g., float32) and normalize\n    data = data.astype(np.float32)  # Convert to float32\n    data = data / 32768.0         # Normalize if it was 16-bit integer audio (adjust if needed)\n                                   # 32768.0 is 2**15, the max positive value for int16\n\n    return librosa.resample(data, orig_sr=samplerate if samplerate else sr, target_sr=sr) # Use 'sr' from function argument, not potentially undefined 'samplerate' from sf.read"
}